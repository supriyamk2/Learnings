{
  
    
        "post0": {
            "title": "Analytics Vidhya  - hackathon",
            "content": "import os os.listdir() . [&#39;.ipynb_checkpoints&#39;, &#39;AV_hack.html&#39;, &#39;AV_hack.ipynb&#39;, &#39;AV_hack2.html&#39;, &#39;AV_hack2.ipynb&#39;, &#39;Untitled.html&#39;, &#39;Untitled.ipynb&#39;, &#39;results_exp.csv&#39;, &#39;results_f.csv&#39;, &#39;sample_submission_pn2DrMq.csv&#39;, &#39;test_QkPvNLx.csv&#39;, &#39;train.csv&#39;, &#39;models&#39;, &#39;results_3m.csv&#39;, &#39;results_3m_less.csv&#39;, &#39;results_avg3.csv&#39;, &#39;sales_down.csv&#39;, &#39;sales_up.csv&#39;, &#39;results_b.csv&#39;, &#39;results_b_up.csv&#39;] . from fastai2.tabular.all import * . path = Path() . path.ls() . (#20) [Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;AV_hack.html&#39;),Path(&#39;AV_hack.ipynb&#39;),Path(&#39;AV_hack2.html&#39;),Path(&#39;AV_hack2.ipynb&#39;),Path(&#39;Untitled.html&#39;),Path(&#39;Untitled.ipynb&#39;),Path(&#39;results_exp.csv&#39;),Path(&#39;results_f.csv&#39;),Path(&#39;sample_submission_pn2DrMq.csv&#39;)...] . train_df = pd.read_csv(&#39;train.csv&#39;) test_df = pd.read_csv(&#39;test_QkPvNLx.csv&#39;) submission = pd.read_csv(&#39;sample_submission_pn2DrMq.csv&#39;) . train_df.head() . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales . 0 1 | 1 | 1 | Development | Course | 0 | 1 | 1 | 11004 | 0.007 | 81 | . 1 2 | 2 | 1 | Development | Course | 0 | 0 | 1 | 13650 | 0.007 | 79 | . 2 3 | 3 | 1 | Development | Course | 0 | 0 | 1 | 11655 | 0.007 | 75 | . 3 4 | 4 | 1 | Development | Course | 0 | 0 | 1 | 12054 | 0.007 | 80 | . 4 5 | 5 | 1 | Development | Course | 0 | 0 | 1 | 6804 | 0.007 | 41 | . train_df.shape . (512087, 11) . train_df[&#39;Sales&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7eb1e4ea90&gt; . train_df.describe() . ID Day_No Course_ID Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales . count 512087.000000 | 512087.000000 | 512087.000000 | 512087.000000 | 512087.000000 | 512087.000000 | 512087.000000 | 510323.000000 | 512087.000000 | . mean 274007.300650 | 434.917869 | 300.388344 | 0.380244 | 0.031639 | 0.488968 | 15375.101198 | 0.073345 | 120.826924 | . std 158228.834029 | 256.044161 | 173.365787 | 0.485447 | 0.175038 | 0.499879 | 7727.231205 | 0.100115 | 54.355258 | . min 1.000000 | 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 168.000000 | 0.000000 | 0.000000 | . 25% 136962.500000 | 214.000000 | 150.000000 | 0.000000 | 0.000000 | 0.000000 | 10584.000000 | 0.010000 | 84.000000 | . 50% 273984.000000 | 427.000000 | 300.000000 | 0.000000 | 0.000000 | 0.000000 | 13776.000000 | 0.035000 | 111.000000 | . 75% 411065.500000 | 658.000000 | 451.000000 | 1.000000 | 0.000000 | 1.000000 | 18123.000000 | 0.094000 | 146.000000 | . max 548027.000000 | 882.000000 | 600.000000 | 1.000000 | 1.000000 | 1.000000 | 100002.000000 | 0.768000 | 682.000000 | . import pandas_profiling . # profile = pandas_profiling.ProfileReport(train_df,minimal=True, # title=&#39;Training&#39;, # html={&#39;style&#39;:{&#39;full_width&#39;:True}}) . # test_profile = pandas_profiling.ProfileReport(test_df,minimal=True, # title=&#39;Test&#39;, # html={&#39;style&#39;:{&#39;full_width&#39;:True}}) . More features . train_df.head(1) . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales . 0 1 | 1 | 1 | Development | Course | 0 | 1 | 1 | 11004 | 0.007 | 81 | . plt.scatter(train_df[&#39;Sales&#39;], train_df[&#39;User_Traffic&#39;]) . &lt;matplotlib.collections.PathCollection at 0x7f7ec3dfb810&gt; . plt.scatter(np.exp(train_df[&#39;Competition_Metric&#39;]),train_df[&#39;Sales&#39;]) . &lt;matplotlib.collections.PathCollection at 0x7f7ec3e16150&gt; . train_df.head() . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales . 0 1 | 1 | 1 | Development | Course | 0 | 1 | 1 | 11004 | 0.007 | 81 | . 1 2 | 2 | 1 | Development | Course | 0 | 0 | 1 | 13650 | 0.007 | 79 | . 2 3 | 3 | 1 | Development | Course | 0 | 0 | 1 | 11655 | 0.007 | 75 | . 3 4 | 4 | 1 | Development | Course | 0 | 0 | 1 | 12054 | 0.007 | 80 | . 4 5 | 5 | 1 | Development | Course | 0 | 0 | 1 | 6804 | 0.007 | 41 | . train_df[train_df[&#39;Course_ID&#39;] ==200][&#39;Sales&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7ec3d8a090&gt; . train_df[train_df[&#39;Course_ID&#39;] ==500][&#39;Sales&#39;][-100:].mean() . 98.38 . train_df . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales . 0 1 | 1 | 1 | Development | Course | 0 | 1 | 1 | 11004 | 0.007 | 81 | . 1 2 | 2 | 1 | Development | Course | 0 | 0 | 1 | 13650 | 0.007 | 79 | . 2 3 | 3 | 1 | Development | Course | 0 | 0 | 1 | 11655 | 0.007 | 75 | . 3 4 | 4 | 1 | Development | Course | 0 | 0 | 1 | 12054 | 0.007 | 80 | . 4 5 | 5 | 1 | Development | Course | 0 | 0 | 1 | 6804 | 0.007 | 41 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 512082 548023 | 878 | 600 | Software Marketing | Program | 0 | 0 | 1 | 8904 | 0.070 | 114 | . 512083 548024 | 879 | 600 | Software Marketing | Program | 0 | 0 | 1 | 10542 | 0.070 | 145 | . 512084 548025 | 880 | 600 | Software Marketing | Program | 0 | 0 | 1 | 13671 | 0.070 | 167 | . 512085 548026 | 881 | 600 | Software Marketing | Program | 0 | 0 | 1 | 8904 | 0.070 | 107 | . 512086 548027 | 882 | 600 | Software Marketing | Program | 1 | 0 | 1 | 11445 | 0.070 | 152 | . 512087 rows × 11 columns . train_df.groupby(&#39;Day_No&#39;).agg(day_mean=(&#39;Sales&#39;,&#39;mean&#39;))[&#39;day_mean&#39;][:25] . Day_No 1 131.223706 2 112.315000 3 102.875000 4 107.440000 5 98.490000 6 94.880000 7 176.696667 8 141.360000 9 125.995000 10 125.900000 11 130.505000 12 99.203333 13 117.456667 14 94.540000 15 88.553333 16 85.163333 17 86.760000 18 98.828333 19 99.086667 20 80.658333 21 141.805000 22 134.098333 23 125.525000 24 120.881667 25 125.545000 Name: day_mean, dtype: float64 . train_df.groupby(&#39;Day_No&#39;).agg(day_mean=(&#39;Sales&#39;,&#39;mean&#39;))[&#39;day_mean&#39;][:30].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7ec3d99610&gt; . train_df.groupby(&#39;Day_No&#39;).agg(day_mean=(&#39;Sales&#39;,&#39;mean&#39;))[&#39;day_mean&#39;][366:730].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7ec3c35310&gt; . plt.plot(train_df[&#39;Sales&#39;]) . [&lt;matplotlib.lines.Line2D at 0x7f7e9e8f77d0&gt;] . # train_df[&#39;day&#39;] = train_df[&#39;Day_No&#39;] %14 # test_df[&#39;day&#39;] = test_df[&#39;Day_No&#39;] %14 . train_df.sort_values(&#39;Day_No&#39;,inplace=True) . train_df.reset_index(drop=True,inplace=True) . train_df[&#39;Day_No&#39;].value_counts() . 767 600 335 600 846 600 334 600 78 600 ... 687 507 581 507 686 507 582 507 695 507 Name: Day_No, Length: 882, dtype: int64 . pd.date_range(&#39;1 Jan 2017&#39;,periods=942,freq=&#39;D&#39; ) . DatetimeIndex([&#39;2017-01-01&#39;, &#39;2017-01-02&#39;, &#39;2017-01-03&#39;, &#39;2017-01-04&#39;, &#39;2017-01-05&#39;, &#39;2017-01-06&#39;, &#39;2017-01-07&#39;, &#39;2017-01-08&#39;, &#39;2017-01-09&#39;, &#39;2017-01-10&#39;, ... &#39;2019-07-22&#39;, &#39;2019-07-23&#39;, &#39;2019-07-24&#39;, &#39;2019-07-25&#39;, &#39;2019-07-26&#39;, &#39;2019-07-27&#39;, &#39;2019-07-28&#39;, &#39;2019-07-29&#39;, &#39;2019-07-30&#39;, &#39;2019-07-31&#39;], dtype=&#39;datetime64[ns]&#39;, length=942, freq=&#39;D&#39;) . test_df[&#39;Day_No&#39;].max() . 942 . date_ids = {} for i,v in enumerate(pd.date_range(&#39;1 Jan 2017&#39;,periods=942,freq=&#39;D&#39; ),1): date_ids[i] =v . train_df[&#39;date&#39;] = train_df[&#39;Day_No&#39;].replace(date_ids) test_df[&#39;date&#39;] = test_df[&#39;Day_No&#39;].replace(date_ids) . train_df[&#39;month&#39;] = train_df[&#39;date&#39;].dt.month train_df[&#39;week&#39;] = train_df[&#39;date&#39;].dt.week train_df[&#39;weekday&#39;] = train_df[&#39;date&#39;].dt.weekday train_df[&#39;is_month_start&#39;] = train_df[&#39;date&#39;].dt.is_month_start train_df[&#39;is_month_end&#39;] = train_df[&#39;date&#39;].dt.is_month_end . test_df[&#39;month&#39;] = test_df[&#39;date&#39;].dt.month test_df[&#39;week&#39;] = test_df[&#39;date&#39;].dt.week test_df[&#39;weekday&#39;] = test_df[&#39;date&#39;].dt.weekday test_df[&#39;is_month_start&#39;] = test_df[&#39;date&#39;].dt.is_month_start test_df[&#39;is_month_end&#39;] = test_df[&#39;date&#39;].dt.is_month_end . train_df[&#39;year&#39;] = train_df[&#39;date&#39;].dt.year test_df[&#39;year&#39;] = test_df[&#39;date&#39;].dt.year . ch = train_df[train_df[&#39;month&#39;].isin([3,4])].copy() . import seaborn as sns . ch[&#39;year&#39;].value_counts() . 2019 36600 2018 36600 2017 36600 Name: year, dtype: int64 . train_df.groupby([&#39;year&#39;,&#39;month&#39;]).agg(year_avg=(&#39;Sales&#39;,&#39;mean&#39;)).reset_index() . year month year_avg . 0 2017 | 1 | 111.106834 | . 1 2017 | 2 | 113.274940 | . 2 2017 | 3 | 125.963226 | . 3 2017 | 4 | 112.324389 | . 4 2017 | 5 | 119.838602 | . 5 2017 | 6 | 114.266444 | . 6 2017 | 7 | 122.075430 | . 7 2017 | 8 | 117.111129 | . 8 2017 | 9 | 112.754333 | . 9 2017 | 10 | 113.051344 | . 10 2017 | 11 | 120.730111 | . 11 2017 | 12 | 142.109462 | . 12 2018 | 1 | 110.481720 | . 13 2018 | 2 | 117.416012 | . 14 2018 | 3 | 115.976290 | . 15 2018 | 4 | 122.981889 | . 16 2018 | 5 | 120.555914 | . 17 2018 | 6 | 124.285611 | . 18 2018 | 7 | 122.292995 | . 19 2018 | 8 | 119.723611 | . 20 2018 | 9 | 119.714004 | . 21 2018 | 10 | 120.057072 | . 22 2018 | 11 | 132.344379 | . 23 2018 | 12 | 144.890628 | . 24 2019 | 1 | 116.441774 | . 25 2019 | 2 | 118.391845 | . 26 2019 | 3 | 123.589570 | . 27 2019 | 4 | 125.905611 | . 28 2019 | 5 | 125.948172 | . 29 2019 | 6 | 190.233333 | . sns.boxplot(x=&#39;year&#39;, y=&#39;Sales&#39;, data=ch) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7e73f67510&gt; . train_df . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales date month week weekday is_month_start is_month_end year . 0 1 | 1 | 1 | Development | Course | 0 | 1 | 1 | 11004 | 0.007 | 81 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 1 139895 | 1 | 154 | Development | Program | 0 | 1 | 1 | 13671 | 0.001 | 127 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 2 415937 | 1 | 457 | Development | Course | 0 | 1 | 0 | 22848 | 0.022 | 228 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 3 140837 | 1 | 155 | Development | Course | 0 | 1 | 0 | 20685 | 0.005 | 160 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 4 304671 | 1 | 334 | Development | Course | 0 | 1 | 0 | 15834 | 0.035 | 117 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 512082 472808 | 882 | 518 | Software Marketing | Program | 1 | 0 | 0 | 26166 | 0.018 | 266 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 512083 174158 | 882 | 191 | Development | Program | 1 | 0 | 0 | 12894 | 0.510 | 115 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 512084 175100 | 882 | 192 | Development | Program | 1 | 0 | 0 | 56637 | 0.003 | 410 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 512085 168690 | 882 | 185 | Software Marketing | Program | 1 | 0 | 1 | 8568 | 0.029 | 100 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 512086 548027 | 882 | 600 | Software Marketing | Program | 1 | 0 | 1 | 11445 | 0.070 | 152 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 512087 rows × 18 columns . train_df[train_df[&#39;Public_Holiday&#39;] ==1].sample(20) . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales date month week weekday is_month_start is_month_end year . 375382 537792 | 641 | 589 | Software Marketing | Course | 1 | 1 | 0 | 21483 | 0.069 | 171 | 2018-10-03 00:00:00 | 10 | 40 | 2 | False | False | 2018 | . 417783 296526 | 724 | 324 | Software Marketing | Program | 0 | 1 | 1 | 9786 | 0.056 | 102 | 2018-12-25 00:00:00 | 12 | 52 | 1 | False | False | 2018 | . 314840 261451 | 525 | 286 | Development | Course | 0 | 1 | 1 | 8841 | 0.197 | 74 | 2018-06-09 00:00:00 | 6 | 23 | 5 | False | False | 2018 | . 83792 374216 | 140 | 411 | Software Marketing | Program | 0 | 1 | 0 | 11550 | 0.231 | 89 | 2017-05-20 00:00:00 | 5 | 20 | 5 | False | False | 2017 | . 308163 161912 | 514 | 178 | Development | Course | 0 | 1 | 1 | 13734 | 0.003 | 85 | 2018-05-29 00:00:00 | 5 | 22 | 1 | False | False | 2018 | . 420969 508290 | 731 | 557 | Development | Course | 0 | 1 | 0 | 4284 | 0.101 | 32 | 2019-01-01 00:00:00 | 1 | 1 | 1 | True | False | 2019 | . 500821 284604 | 864 | 311 | Development | Program | 0 | 1 | 0 | 15561 | 0.008 | 91 | 2019-05-14 00:00:00 | 5 | 20 | 1 | False | False | 2019 | . 83865 500531 | 140 | 549 | Finance &amp; Accounting | Program | 0 | 1 | 1 | 8862 | 0.007 | 66 | 2017-05-20 00:00:00 | 5 | 20 | 5 | False | False | 2017 | . 478001 100950 | 826 | 111 | Software Marketing | Program | 0 | 1 | 1 | 4578 | 0.036 | 66 | 2019-04-06 00:00:00 | 4 | 14 | 5 | False | False | 2019 | . 507881 227683 | 875 | 249 | Development | Course | 0 | 1 | 0 | 5943 | 0.012 | 40 | 2019-05-25 00:00:00 | 5 | 21 | 5 | False | False | 2019 | . 418118 284465 | 725 | 311 | Development | Program | 0 | 1 | 0 | 2730 | 0.008 | 14 | 2018-12-26 00:00:00 | 12 | 52 | 2 | False | False | 2018 | . 507486 204869 | 875 | 224 | Finance &amp; Accounting | Program | 0 | 1 | 0 | 11529 | 0.344 | 109 | 2019-05-25 00:00:00 | 5 | 21 | 5 | False | False | 2019 | . 492906 202019 | 851 | 221 | Software Marketing | Course | 1 | 1 | 0 | 8547 | 0.087 | 57 | 2019-05-01 00:00:00 | 5 | 18 | 2 | True | False | 2019 | . 417994 414961 | 725 | 455 | Development | Program | 0 | 1 | 0 | 2100 | 0.009 | 15 | 2018-12-26 00:00:00 | 12 | 52 | 2 | False | False | 2018 | . 215232 445703 | 359 | 489 | Development | Program | 0 | 1 | 0 | 12012 | 0.042 | 100 | 2017-12-25 00:00:00 | 12 | 52 | 0 | False | False | 2017 | . 307983 431338 | 514 | 473 | Finance &amp; Accounting | Course | 0 | 1 | 1 | 16359 | 0.082 | 102 | 2018-05-29 00:00:00 | 5 | 22 | 1 | False | False | 2018 | . 417888 126673 | 725 | 139 | Software Marketing | Course | 0 | 1 | 0 | 4578 | 0.057 | 36 | 2018-12-26 00:00:00 | 12 | 52 | 2 | False | False | 2018 | . 165244 292310 | 276 | 320 | Development | Course | 0 | 1 | 1 | 14868 | 0.009 | 87 | 2017-10-03 00:00:00 | 10 | 40 | 1 | False | False | 2017 | . 215212 216599 | 359 | 237 | Software Marketing | Program | 0 | 1 | 1 | 11382 | 0.059 | 123 | 2017-12-25 00:00:00 | 12 | 52 | 0 | False | False | 2017 | . 89564 535417 | 150 | 587 | Development | Course | 1 | 1 | 1 | 15603 | 0.002 | 105 | 2017-05-30 00:00:00 | 5 | 22 | 1 | False | False | 2017 | . train_df[&#39;month&#39;].value_counts() . 5 55800 3 55800 1 55799 4 54000 2 50400 6 36600 12 34317 10 34317 8 34317 7 34317 11 33210 9 33210 Name: month, dtype: int64 . test_df.head() . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric date month week weekday is_month_start is_month_end year . 0 883 | 883 | 1 | Development | Course | 1 | 0 | 1 | 0.007 | 2019-06-02 | 6 | 22 | 6 | False | False | 2019 | . 1 884 | 884 | 1 | Development | Course | 1 | 0 | 1 | 0.007 | 2019-06-03 | 6 | 23 | 0 | False | False | 2019 | . 2 885 | 885 | 1 | Development | Course | 1 | 0 | 1 | 0.007 | 2019-06-04 | 6 | 23 | 1 | False | False | 2019 | . 3 886 | 886 | 1 | Development | Course | 1 | 0 | 1 | 0.007 | 2019-06-05 | 6 | 23 | 2 | False | False | 2019 | . 4 887 | 887 | 1 | Development | Course | 0 | 0 | 1 | 0.007 | 2019-06-06 | 6 | 23 | 3 | False | False | 2019 | . test_df[&#39;month&#39;].value_counts() . 7 18600 6 17400 Name: month, dtype: int64 . train_df[train_df[&#39;Public_Holiday&#39;]==1][:30] . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales date month week weekday is_month_start is_month_end year . 0 1 | 1 | 1 | Development | Course | 0 | 1 | 1 | 11004 | 0.007 | 81 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 1 139895 | 1 | 154 | Development | Program | 0 | 1 | 1 | 13671 | 0.001 | 127 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 2 415937 | 1 | 457 | Development | Course | 0 | 1 | 0 | 22848 | 0.022 | 228 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 3 140837 | 1 | 155 | Development | Course | 0 | 1 | 0 | 20685 | 0.005 | 160 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 4 304671 | 1 | 334 | Development | Course | 0 | 1 | 0 | 15834 | 0.035 | 117 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 5 141779 | 1 | 156 | Development | Course | 0 | 1 | 0 | 52038 | 0.007 | 217 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 6 486630 | 1 | 534 | Software Marketing | Course | 0 | 1 | 0 | 9471 | 0.086 | 94 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 7 142721 | 1 | 157 | Software Marketing | Program | 0 | 1 | 1 | 16548 | 0.202 | 96 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 8 303729 | 1 | 333 | Development | Course | 0 | 1 | 0 | 27258 | 0.095 | 159 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 9 143663 | 1 | 158 | Development | Program | 0 | 1 | 1 | 65793 | 0.015 | 398 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 10 487572 | 1 | 535 | Software Marketing | Program | 0 | 1 | 1 | 15246 | 0.077 | 138 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 11 485688 | 1 | 533 | Software Marketing | Program | 0 | 1 | 1 | 11949 | 0.045 | 77 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 12 144421 | 1 | 159 | Development | Program | 0 | 1 | 1 | 10962 | 0.004 | 99 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 13 302787 | 1 | 332 | Development | Program | 0 | 1 | 0 | 17052 | 0.002 | 141 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 14 416879 | 1 | 458 | Development | Course | 0 | 1 | 1 | 16338 | 0.005 | 123 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 15 145363 | 1 | 160 | Finance &amp; Accounting | Course | 0 | 1 | 0 | 10332 | 0.032 | 79 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 16 484746 | 1 | 532 | Software Marketing | Course | 0 | 1 | 0 | 12810 | 0.276 | 105 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 17 146305 | 1 | 161 | Development | Course | 0 | 1 | 0 | 26061 | 0.059 | 201 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 18 301845 | 1 | 331 | Software Marketing | Course | 0 | 1 | 0 | 9681 | 0.167 | 73 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 19 147247 | 1 | 162 | Development | Program | 0 | 1 | 0 | 9912 | 0.001 | 71 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 20 11121 | 1 | 13 | Development | Program | 0 | 1 | 1 | 14910 | 0.042 | 130 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 21 239997 | 1 | 263 | Software Marketing | Program | 0 | 1 | 0 | 15750 | 0.101 | 125 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 22 305613 | 1 | 335 | Development | Program | 0 | 1 | 0 | 13776 | 0.003 | 86 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 23 138953 | 1 | 153 | Software Marketing | Course | 0 | 1 | 1 | 14826 | 0.004 | 113 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 24 12063 | 1 | 14 | Development | Course | 0 | 1 | 1 | 11277 | 0.013 | 121 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 25 131601 | 1 | 145 | Software Marketing | Course | 0 | 1 | 1 | 12768 | 0.090 | 113 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 26 132543 | 1 | 146 | Development | Course | 0 | 1 | 1 | 7560 | 0.004 | 70 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 27 12821 | 1 | 15 | Development | Program | 0 | 1 | 1 | 10080 | 0.060 | 100 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 28 309381 | 1 | 339 | Development | Course | 0 | 1 | 0 | 24990 | 0.023 | 175 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 29 414237 | 1 | 455 | Development | Program | 0 | 1 | 0 | 12285 | 0.009 | 112 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . import seaborn as sns . sns.boxplot(x=train_df[&#39;month&#39;], y=train_df[&#39;Sales&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7e84070b10&gt; . train_df.columns . Index([&#39;ID&#39;, &#39;Day_No&#39;, &#39;Course_ID&#39;, &#39;Course_Domain&#39;, &#39;Course_Type&#39;, &#39;Short_Promotion&#39;, &#39;Public_Holiday&#39;, &#39;Long_Promotion&#39;, &#39;User_Traffic&#39;, &#39;Competition_Metric&#39;, &#39;Sales&#39;, &#39;date&#39;, &#39;month&#39;, &#39;week&#39;, &#39;weekday&#39;, &#39;is_month_start&#39;, &#39;is_month_end&#39;, &#39;year&#39;], dtype=&#39;object&#39;) . sns.boxplot(x=train_df[&#39;weekday&#39;], y=train_df[&#39;Sales&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7e73d951d0&gt; . train_df.sample(14) . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales date month weekofyear week weekday is_month_start is_month_end . 85131 326110 | 142 | 357 | Finance &amp; Accounting | Program | 0 | 0 | 0 | 9534 | 0.052 | 73 | 2017-05-22 00:00:00 | 5 | 21 | 21 | 0 | False | False | . 407233 330440 | 704 | 361 | Software Marketing | Course | 1 | 0 | 0 | 19635 | 0.245 | 190 | 2018-12-05 00:00:00 | 12 | 49 | 49 | 2 | False | False | . 106501 158934 | 178 | 175 | Software Marketing | Course | 0 | 0 | 1 | 6867 | 0.010 | 75 | 2017-06-27 00:00:00 | 6 | 26 | 26 | 1 | False | False | . 12280 288287 | 21 | 316 | Finance &amp; Accounting | Program | 1 | 0 | 0 | 9975 | 0.016 | 73 | 2017-01-21 00:00:00 | 1 | 3 | 3 | 5 | False | False | . 56438 39497 | 95 | 44 | Software Marketing | Program | 0 | 0 | 1 | 12012 | 0.014 | 106 | 2017-04-05 00:00:00 | 4 | 14 | 14 | 2 | False | False | . 42970 2898 | 72 | 4 | Development | Course | 0 | 0 | 0 | 11130 | 0.099 | 71 | 2017-03-13 00:00:00 | 3 | 11 | 11 | 0 | False | False | . 347277 263211 | 585 | 288 | Software Marketing | Program | 1 | 0 | 0 | 17304 | 0.036 | 144 | 2018-08-08 00:00:00 | 8 | 32 | 32 | 2 | False | False | . 121180 222668 | 202 | 244 | Development | Course | 0 | 0 | 0 | 14763 | 0.003 | 95 | 2017-07-21 00:00:00 | 7 | 29 | 29 | 4 | False | False | . 507393 7469 | 875 | 8 | Development | Program | 0 | 1 | 1 | 16758 | 0.014 | 110 | 2019-05-25 00:00:00 | 5 | 21 | 21 | 5 | False | False | . 258088 117143 | 431 | 129 | Finance &amp; Accounting | Course | 1 | 0 | 1 | 20895 | 0.002 | 162 | 2018-03-07 00:00:00 | 3 | 10 | 10 | 2 | False | False | . 339446 374646 | 570 | 411 | Software Marketing | Program | 0 | 0 | 0 | 10479 | 0.231 | 79 | 2018-07-24 00:00:00 | 7 | 30 | 30 | 1 | False | False | . 64958 278197 | 109 | 305 | Development | Program | 0 | 0 | 0 | 17955 | 0.539 | 131 | 2017-04-19 00:00:00 | 4 | 16 | 16 | 2 | False | False | . 273400 134698 | 456 | 148 | Development | Program | 1 | 0 | 0 | 24948 | 0.017 | 189 | 2018-04-01 00:00:00 | 4 | 13 | 13 | 6 | True | False | . 85166 465842 | 142 | 511 | Development | Course | 0 | 0 | 1 | 9849 | 0.016 | 66 | 2017-05-22 00:00:00 | 5 | 21 | 21 | 0 | False | False | . test_df.sample(10) . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric date month weekofyear week weekday is_month_start is_month_end . 22384 342317 | 887 | 374 | Finance &amp; Accounting | Course | 0 | 0 | 0 | 0.091 | 2019-06-06 | 6 | 23 | 23 | 3 | False | False | . 30472 463809 | 935 | 508 | Development | Course | 0 | 0 | 0 | 0.037 | 2019-07-24 | 7 | 30 | 30 | 2 | False | False | . 19889 303698 | 912 | 332 | Development | Program | 1 | 0 | 0 | 0.002 | 2019-07-01 | 7 | 27 | 27 | 0 | True | False | . 917 14662 | 900 | 16 | Development | Course | 1 | 0 | 1 | 0.001 | 2019-06-19 | 6 | 25 | 25 | 2 | False | False | . 23276 354821 | 939 | 388 | Software Marketing | Program | 1 | 0 | 0 | 0.036 | 2019-07-28 | 7 | 30 | 30 | 6 | False | False | . 33031 503005 | 914 | 551 | Software Marketing | Program | 1 | 0 | 1 | 0.036 | 2019-07-03 | 7 | 27 | 27 | 2 | False | False | . 35040 534266 | 883 | 585 | Development | Course | 1 | 0 | 1 | 0.002 | 2019-06-02 | 6 | 22 | 22 | 6 | False | False | . 30471 463808 | 934 | 508 | Development | Course | 0 | 0 | 0 | 0.037 | 2019-07-23 | 7 | 30 | 30 | 1 | False | False | . 32333 492275 | 936 | 539 | Development | Course | 0 | 0 | 1 | 0.002 | 2019-07-25 | 7 | 30 | 30 | 3 | False | False | . 32519 494923 | 942 | 542 | Finance &amp; Accounting | Program | 1 | 0 | 1 | 0.051 | 2019-07-31 | 7 | 31 | 31 | 2 | False | True | . train_df[train_df[&#39;Sales&#39;]&gt;500] . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales date month week weekday is_month_start is_month_end year . 53366 277235 | 89 | 304 | Finance &amp; Accounting | Program | 0 | 0 | 0 | 60921 | 0.000 | 504 | 2017-03-30 00:00:00 | 3 | 13 | 3 | False | False | 2017 | . 90261 137219 | 151 | 151 | Development | Program | 1 | 0 | 0 | 67620 | 0.004 | 524 | 2017-05-31 00:00:00 | 5 | 22 | 2 | False | True | 2017 | . 176465 67405 | 295 | 74 | Software Marketing | Course | 1 | 0 | 1 | 11634 | 0.044 | 554 | 2017-10-22 00:00:00 | 10 | 42 | 6 | False | False | 2017 | . 177868 67407 | 297 | 74 | Software Marketing | Course | 1 | 0 | 1 | 12054 | 0.044 | 599 | 2017-10-24 00:00:00 | 10 | 43 | 1 | False | False | 2017 | . 201105 174554 | 336 | 192 | Development | Program | 1 | 0 | 0 | 66822 | 0.003 | 517 | 2017-12-02 00:00:00 | 12 | 48 | 5 | False | False | 2017 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 415963 306333 | 721 | 335 | Development | Program | 0 | 0 | 0 | 93618 | 0.003 | 523 | 2018-12-22 00:00:00 | 12 | 51 | 5 | False | False | 2018 | . 416270 137789 | 721 | 151 | Development | Program | 0 | 0 | 0 | 89544 | 0.004 | 616 | 2018-12-22 00:00:00 | 12 | 51 | 5 | False | False | 2018 | . 416625 386860 | 722 | 424 | Development | Program | 0 | 0 | 1 | 77868 | 0.000 | 511 | 2018-12-23 00:00:00 | 12 | 51 | 6 | False | False | 2018 | . 416773 137790 | 722 | 151 | Development | Program | 0 | 0 | 0 | 87780 | 0.004 | 579 | 2018-12-23 00:00:00 | 12 | 51 | 6 | False | False | 2018 | . 512026 137950 | 882 | 151 | Development | Program | 1 | 0 | 0 | 62454 | 0.004 | 514 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 61 rows × 18 columns . train_df[&#39;Sales&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7e73cfa150&gt; . train_df = train_df[(train_df[&#39;Sales&#39;]&lt;410) &amp; (train_df[&#39;Sales&#39;]&gt;0)] . train_df.shape, test_df.shape . ((511573, 18), (36000, 16)) . train_df.head(2) . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion User_Traffic Competition_Metric Sales date month week weekday is_month_start is_month_end year . 0 1 | 1 | 1 | Development | Course | 0 | 1 | 1 | 11004 | 0.007 | 81 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 1 139895 | 1 | 154 | Development | Program | 0 | 1 | 1 | 13671 | 0.001 | 127 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . train_df=train_df.drop([&#39;ID&#39;, &#39;User_Traffic&#39;],axis=1) . train_df.head(2) . Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric Sales date month week weekday is_month_start is_month_end year . 0 1 | 1 | Development | Course | 0 | 1 | 1 | 0.007 | 81 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 1 1 | 154 | Development | Program | 0 | 1 | 1 | 0.001 | 127 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . #test_df.drop([&#39;ID&#39;],axis=1,inplace=True) test_df.head(2) . ID Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric date month week weekday is_month_start is_month_end year . 0 883 | 883 | 1 | Development | Course | 1 | 0 | 1 | 0.007 | 2019-06-02 | 6 | 22 | 6 | False | False | 2019 | . 1 884 | 884 | 1 | Development | Course | 1 | 0 | 1 | 0.007 | 2019-06-03 | 6 | 23 | 0 | False | False | 2019 | . train_df.tail() . Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric Sales date month week weekday is_month_start is_month_end year . 512081 882 | 190 | Software Marketing | Course | 1 | 0 | 0 | 0.149 | 163 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 512082 882 | 518 | Software Marketing | Program | 1 | 0 | 0 | 0.018 | 266 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 512083 882 | 191 | Development | Program | 1 | 0 | 0 | 0.510 | 115 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 512085 882 | 185 | Software Marketing | Program | 1 | 0 | 1 | 0.029 | 100 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 512086 882 | 600 | Software Marketing | Program | 1 | 0 | 1 | 0.070 | 152 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . train_df = train_df.sort_values(by=&#39;Day_No&#39;) . train_df . Day_No Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric Sales date month week weekday is_month_start is_month_end year . 0 1 | 1 | Development | Course | 0 | 1 | 1 | 0.007 | 81 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 396 1 | 483 | Development | Course | 0 | 1 | 0 | 0.042 | 105 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 397 1 | 582 | Software Marketing | Program | 0 | 1 | 1 | 0.009 | 77 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 398 1 | 565 | Software Marketing | Program | 0 | 1 | 1 | 0.024 | 86 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . 399 1 | 61 | Development | Course | 0 | 1 | 0 | 0.005 | 76 | 2017-01-01 00:00:00 | 1 | 52 | 6 | True | False | 2017 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 511688 882 | 91 | Software Marketing | Course | 1 | 0 | 0 | 0.112 | 227 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 511689 882 | 92 | Development | Program | 1 | 0 | 0 | 0.039 | 200 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 511690 882 | 560 | Software Marketing | Course | 1 | 0 | 0 | 0.228 | 152 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 511692 882 | 94 | Development | Program | 1 | 0 | 1 | 0.001 | 137 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 512086 882 | 600 | Software Marketing | Program | 1 | 0 | 1 | 0.070 | 152 | 2019-06-01 00:00:00 | 6 | 22 | 5 | True | False | 2019 | . 511573 rows × 16 columns . train_df.drop([&#39;week&#39;],axis=1,inplace=True) test_df.drop([&#39;week&#39;],axis=1,inplace=True) . train_df[train_df[&#39;Day_No&#39;]&gt;860][&#39;Course_ID&#39;].value_counts() . 599 22 458 22 442 22 434 22 426 22 .. 335 21 200 21 192 21 340 21 151 19 Name: Course_ID, Length: 600, dtype: int64 . train_df.drop([&#39;Day_No&#39;,&#39;date&#39;],axis=1,inplace=True) test_df.drop([&#39;Day_No&#39;,&#39;date&#39;],axis=1,inplace=True) . submission.head(2) . ID Sales . 0 883 | 5 | . 1 884 | 5 | . submission.shape . (36000, 2) . Model . train_df.head(1) . Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric Sales month weekday is_month_start is_month_end year . 0 1 | Development | Course | 0 | 1 | 1 | 0.007 | 81 | 1 | 6 | True | False | 2017 | . train_df[&#39;Sales&#39;] = np.log(train_df[&#39;Sales&#39;]) . train_df.head(1) . Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric Sales month weekday is_month_start is_month_end year . 0 1 | Development | Course | 0 | 1 | 1 | 0.007 | 4.394449 | 1 | 6 | True | False | 2017 | . train_df.reset_index(drop=True,inplace=True) . train_df.head() . Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric Sales month weekday is_month_start is_month_end year . 0 1 | Development | Course | 0 | 1 | 1 | 0.007 | 4.394449 | 1 | 6 | True | False | 2017 | . 1 483 | Development | Course | 0 | 1 | 0 | 0.042 | 4.653960 | 1 | 6 | True | False | 2017 | . 2 582 | Software Marketing | Program | 0 | 1 | 1 | 0.009 | 4.343805 | 1 | 6 | True | False | 2017 | . 3 565 | Software Marketing | Program | 0 | 1 | 1 | 0.024 | 4.454347 | 1 | 6 | True | False | 2017 | . 4 61 | Development | Course | 0 | 1 | 0 | 0.005 | 4.330733 | 1 | 6 | True | False | 2017 | . dep_var = &#39;Sales&#39; cat_names = [&#39;Course_ID&#39;, &#39;Course_Domain&#39;, &#39;Course_Type&#39;, &#39;Short_Promotion&#39;, &#39;Public_Holiday&#39;, &#39;Long_Promotion&#39;, &#39;month&#39;, &#39;weekday&#39;,&#39;is_month_start&#39;,&#39;is_month_end&#39;] cont_names = [&#39;Competition_Metric&#39;] procs = [Categorify, FillMissing, Normalize] . len(train_df) . 511573 . train_df . Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric Sales month weekday is_month_start is_month_end year . 0 1 | Development | Course | 0 | 1 | 1 | 0.007 | 4.394449 | 1 | 6 | True | False | 2017 | . 1 483 | Development | Course | 0 | 1 | 0 | 0.042 | 4.653960 | 1 | 6 | True | False | 2017 | . 2 582 | Software Marketing | Program | 0 | 1 | 1 | 0.009 | 4.343805 | 1 | 6 | True | False | 2017 | . 3 565 | Software Marketing | Program | 0 | 1 | 1 | 0.024 | 4.454347 | 1 | 6 | True | False | 2017 | . 4 61 | Development | Course | 0 | 1 | 0 | 0.005 | 4.330733 | 1 | 6 | True | False | 2017 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 511568 91 | Software Marketing | Course | 1 | 0 | 0 | 0.112 | 5.424950 | 6 | 5 | True | False | 2019 | . 511569 92 | Development | Program | 1 | 0 | 0 | 0.039 | 5.298317 | 6 | 5 | True | False | 2019 | . 511570 560 | Software Marketing | Course | 1 | 0 | 0 | 0.228 | 5.023881 | 6 | 5 | True | False | 2019 | . 511571 94 | Development | Program | 1 | 0 | 1 | 0.001 | 4.919981 | 6 | 5 | True | False | 2019 | . 511572 600 | Software Marketing | Program | 1 | 0 | 1 | 0.070 | 5.023881 | 6 | 5 | True | False | 2019 | . 511573 rows × 13 columns . 600*90 . 54000 . train_df_3m = train_df[(train_df[&#39;month&#39;].isin([6,7])) &amp; (train_df[&#39;year&#39;]==2018)].copy() . 600*20 . 12000 . train_df_3m[&#39;Course_ID&#39;].value_counts() . 575 61 12 61 522 61 554 61 586 61 .. 29 30 329 30 169 29 551 29 555 29 Name: Course_ID, Length: 600, dtype: int64 . train_df_3m = train_df_3m.sample(frac=1).reset_index(drop=True) . train_df_3m . Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion Competition_Metric Sales month weekday is_month_start is_month_end year . 0 126 | Development | Program | 1 | 0 | 0 | 0.198 | 4.691348 | 6 | 6 | False | False | 2018 | . 1 148 | Development | Program | 0 | 0 | 0 | 0.017 | 5.003946 | 7 | 1 | False | False | 2018 | . 2 432 | Development | Course | 1 | 0 | 0 | 0.272 | 4.934474 | 6 | 0 | False | False | 2018 | . 3 79 | Development | Course | 1 | 0 | 1 | 0.003 | 4.997212 | 6 | 6 | False | False | 2018 | . 4 82 | Development | Program | 1 | 0 | 0 | 0.334 | 5.192957 | 6 | 6 | False | False | 2018 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 33681 374 | Finance &amp; Accounting | Course | 1 | 0 | 0 | 0.091 | 5.087596 | 6 | 6 | False | False | 2018 | . 33682 483 | Development | Course | 1 | 0 | 0 | 0.042 | 4.595120 | 7 | 5 | False | False | 2018 | . 33683 145 | Software Marketing | Course | 0 | 0 | 1 | 0.090 | 3.988984 | 7 | 1 | False | False | 2018 | . 33684 137 | Software Marketing | Program | 1 | 0 | 1 | 0.142 | 4.919981 | 7 | 6 | False | False | 2018 | . 33685 21 | Finance &amp; Accounting | Course | 1 | 0 | 0 | 0.056 | 4.955827 | 6 | 2 | False | False | 2018 | . 33686 rows × 13 columns . splits = IndexSplitter(list(range(30000,len(train_df_3m))))(range_of(train_df_3m)) . to = TabularPandas(train_df_3m, procs, cat_names, cont_names, y_names=&quot;Sales&quot;, splits=splits) . dls = to.dataloaders(bs=64) . dls.show_batch() . Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion month weekday is_month_start is_month_end Competition_Metric_na Competition_Metric Sales . 0 281 | Development | Program | 0 | 0 | 1 | 6 | 2 | False | False | False | 0.030 | 4.553877 | . 1 219 | Development | Program | 1 | 0 | 1 | 6 | 0 | False | False | False | 0.001 | 5.003946 | . 2 318 | Development | Course | 0 | 0 | 0 | 6 | 3 | False | False | False | 0.003 | 4.219508 | . 3 536 | Development | Course | 0 | 0 | 1 | 7 | 4 | False | False | False | 0.010 | 4.234107 | . 4 154 | Development | Program | 0 | 0 | 1 | 6 | 6 | False | False | False | 0.001 | 5.068904 | . 5 452 | Software Marketing | Program | 0 | 0 | 0 | 7 | 3 | False | False | False | 0.127 | 4.584968 | . 6 323 | Development | Course | 0 | 0 | 1 | 7 | 4 | False | False | False | 0.047 | 4.043051 | . 7 426 | Development | Program | 1 | 0 | 0 | 7 | 6 | False | False | False | 0.261 | 4.682131 | . 8 125 | Software Marketing | Course | 1 | 0 | 1 | 7 | 2 | False | False | False | 0.013 | 4.189655 | . 9 450 | Software Marketing | Course | 0 | 0 | 1 | 6 | 2 | False | False | False | 0.003 | 4.553877 | . y_range = torch.tensor([train_df_3m[&#39;Sales&#39;].min(), train_df_3m[&#39;Sales&#39;].max()]) y_range . tensor([2.8332, 6.0137]) . learn = tabular_learner(dls, layers=[64,32], y_range=y_range, metrics=[rmse], loss_func=MSELossFlat()) . learn.summary() . TabularModel (Input shape: [&#39;64 x 11&#39;, &#39;64 x 1&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Embedding 64 x 58 34,858 True ________________________________________________________________ Embedding 64 x 4 20 True ________________________________________________________________ Embedding 64 x 3 12 True ________________________________________________________________ Embedding 64 x 3 9 True ________________________________________________________________ Embedding 64 x 3 9 True ________________________________________________________________ Embedding 64 x 3 9 True ________________________________________________________________ Embedding 64 x 3 9 True ________________________________________________________________ Embedding 64 x 5 40 True ________________________________________________________________ Embedding 64 x 3 9 True ________________________________________________________________ Embedding 64 x 3 9 True ________________________________________________________________ Embedding 64 x 3 9 True ________________________________________________________________ Dropout 64 x 91 0 False ________________________________________________________________ BatchNorm1d 64 x 1 2 True ________________________________________________________________ BatchNorm1d 64 x 92 184 True ________________________________________________________________ Linear 64 x 64 5,888 True ________________________________________________________________ ReLU 64 x 64 0 False ________________________________________________________________ BatchNorm1d 64 x 64 128 True ________________________________________________________________ Linear 64 x 32 2,048 True ________________________________________________________________ ReLU 64 x 32 0 False ________________________________________________________________ Linear 64 x 1 33 True ________________________________________________________________ SigmoidRange 64 x 1 0 False ________________________________________________________________ Total params: 43,276 Total trainable params: 43,276 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f7eb1f333b0&gt; Loss function: FlattenedLoss of MSELoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learn.lr_find() . SuggestedLRs(lr_min=0.2754228591918945, lr_steep=0.0063095735386013985) . learn.fit_one_cycle(10, 1e-2, wd=0.1) . epoch train_loss valid_loss _rmse time . 0 | 0.036919 | 0.032667 | 0.180741 | 00:07 | . 1 | 0.032601 | 0.026184 | 0.161816 | 00:07 | . 2 | 0.027214 | 0.025218 | 0.158801 | 00:07 | . 3 | 0.024333 | 0.022286 | 0.149286 | 00:07 | . 4 | 0.022588 | 0.019946 | 0.141231 | 00:06 | . 5 | 0.020391 | 0.019083 | 0.138142 | 00:06 | . 6 | 0.019126 | 0.017025 | 0.130480 | 00:06 | . 7 | 0.015954 | 0.015540 | 0.124659 | 00:06 | . 8 | 0.014221 | 0.015260 | 0.123531 | 00:06 | . 9 | 0.013616 | 0.014738 | 0.121399 | 00:06 | . learn.show_results() . Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion month weekday is_month_start is_month_end Competition_Metric_na Competition_Metric Sales Sales_pred . 0 501.0 | 2.0 | 1.0 | 2.0 | 1.0 | 1.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 0.207218 | 4.812184 | 4.878877 | . 1 35.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 2.0 | 4.0 | 1.0 | 1.0 | 1.0 | -0.465504 | 4.219508 | 4.139396 | . 2 92.0 | 2.0 | 3.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | -0.346788 | 4.969813 | 4.998371 | . 3 123.0 | 2.0 | 1.0 | 2.0 | 1.0 | 1.0 | 2.0 | 7.0 | 1.0 | 1.0 | 1.0 | 0.157753 | 5.501258 | 5.481016 | . 4 422.0 | 4.0 | 3.0 | 2.0 | 1.0 | 2.0 | 1.0 | 7.0 | 1.0 | 1.0 | 1.0 | -0.663363 | 5.303305 | 5.196049 | . 5 320.0 | 2.0 | 1.0 | 1.0 | 1.0 | 2.0 | 2.0 | 6.0 | 1.0 | 1.0 | 1.0 | -0.643577 | 4.418840 | 4.503658 | . 6 559.0 | 2.0 | 3.0 | 1.0 | 1.0 | 1.0 | 1.0 | 5.0 | 2.0 | 1.0 | 1.0 | -0.712828 | 4.488636 | 4.536578 | . 7 202.0 | 2.0 | 3.0 | 1.0 | 1.0 | 2.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.0 | -0.524861 | 4.744932 | 4.665170 | . 8 106.0 | 4.0 | 3.0 | 1.0 | 1.0 | 1.0 | 2.0 | 5.0 | 1.0 | 1.0 | 1.0 | -0.267644 | 4.682131 | 4.573598 | . learn.lr_find() . SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=1.3182567499825382e-06) . learn.fit_one_cycle(10, 1e-5, wd=0.2) . epoch train_loss valid_loss _rmse time . 0 | 0.011990 | 0.011751 | 0.108404 | 00:14 | . 1 | 0.013155 | 0.011676 | 0.108056 | 00:14 | . 2 | 0.011158 | 0.011701 | 0.108172 | 00:14 | . 3 | 0.011487 | 0.011658 | 0.107974 | 00:14 | . 4 | 0.011663 | 0.011745 | 0.108376 | 00:14 | . 5 | 0.011077 | 0.011677 | 0.108061 | 00:14 | . 6 | 0.011734 | 0.011688 | 0.108111 | 00:14 | . 7 | 0.011738 | 0.011631 | 0.107849 | 00:14 | . 8 | 0.011488 | 0.011717 | 0.108243 | 00:14 | . 9 | 0.013932 | 0.011643 | 0.107903 | 00:14 | . learn.show_results() . Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion weekofyear month weekday is_month_start is_month_end Competition_Metric_na Competition_Metric Sales Sales_pred . 0 276.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 3.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.0 | -0.711633 | 4.369448 | 4.501095 | . 1 505.0 | 2.0 | 3.0 | 2.0 | 1.0 | 1.0 | 8.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.086212 | 5.062595 | 4.939821 | . 2 599.0 | 2.0 | 3.0 | 1.0 | 1.0 | 1.0 | 3.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.485134 | 4.736198 | 4.634775 | . 3 451.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 6.0 | 2.0 | 7.0 | 1.0 | 1.0 | 1.0 | -0.222953 | 4.948760 | 4.985892 | . 4 289.0 | 4.0 | 3.0 | 1.0 | 1.0 | 2.0 | 7.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 0.674622 | 4.477337 | 4.320698 | . 5 269.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 7.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | -0.183061 | 4.543295 | 4.537782 | . 6 117.0 | 2.0 | 1.0 | 2.0 | 1.0 | 1.0 | 6.0 | 2.0 | 3.0 | 1.0 | 1.0 | 1.0 | -0.522145 | 5.043425 | 5.064213 | . 7 33.0 | 2.0 | 3.0 | 1.0 | 1.0 | 2.0 | 1.0 | 1.0 | 4.0 | 2.0 | 1.0 | 1.0 | 0.235808 | 4.543295 | 4.708697 | . 8 429.0 | 2.0 | 3.0 | 1.0 | 1.0 | 1.0 | 5.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.983787 | 4.382027 | 4.471707 | . Make prediction . train_df.columns . Index([&#39;Course_ID&#39;, &#39;Course_Domain&#39;, &#39;Course_Type&#39;, &#39;Short_Promotion&#39;, &#39;Public_Holiday&#39;, &#39;Long_Promotion&#39;, &#39;Competition_Metric&#39;, &#39;Sales&#39;, &#39;month&#39;, &#39;weekday&#39;, &#39;is_month_start&#39;, &#39;is_month_end&#39;, &#39;year&#39;], dtype=&#39;object&#39;) . cols = [&#39;Course_ID&#39;, &#39;Course_Domain&#39;, &#39;Course_Type&#39;, &#39;Short_Promotion&#39;, &#39;Public_Holiday&#39;, &#39;Long_Promotion&#39;, &#39;Competition_Metric&#39;, &#39;month&#39;, &#39;weekday&#39;, &#39;is_month_start&#39;, &#39;is_month_end&#39;] . dl = learn.dls.test_dl(test_df[cols]) . dl.show_batch() . Course_ID Course_Domain Course_Type Short_Promotion Public_Holiday Long_Promotion month weekday is_month_start is_month_end Competition_Metric_na Competition_Metric . 0 1 | Development | Course | 1 | 0 | 1 | 6 | 6 | False | False | False | 0.007 | . 1 1 | Development | Course | 1 | 0 | 1 | 6 | 0 | False | False | False | 0.007 | . 2 1 | Development | Course | 1 | 0 | 1 | 6 | 1 | False | False | False | 0.007 | . 3 1 | Development | Course | 1 | 0 | 1 | 6 | 2 | False | False | False | 0.007 | . 4 1 | Development | Course | 0 | 0 | 1 | 6 | 3 | False | False | False | 0.007 | . 5 1 | Development | Course | 0 | 0 | 1 | 6 | 4 | False | False | False | 0.007 | . 6 1 | Development | Course | 0 | 0 | 1 | 6 | 5 | False | False | False | 0.007 | . 7 1 | Development | Course | 0 | 0 | 1 | 6 | 6 | False | False | False | 0.007 | . 8 1 | Development | Course | 0 | 0 | 1 | 6 | 0 | False | False | False | 0.007 | . 9 1 | Development | Course | 0 | 0 | 1 | 6 | 1 | False | False | False | 0.007 | . op = learn.get_preds(dl=dl) . op_res = op[0].flatten().numpy() . op_res . array([4.6616173, 4.6442842, 4.615026 , ..., 4.836421 , 4.807702 , 4.9143486], dtype=float32) . submission[&#39;Sales&#39;] = np.exp(op_res) submission . ID Sales . 0 883 | 105.807068 | . 1 884 | 103.988907 | . 2 885 | 100.990456 | . 3 886 | 93.665955 | . 4 887 | 49.149513 | . ... ... | ... | . 35995 548083 | 150.104187 | . 35996 548084 | 127.545181 | . 35997 548085 | 126.017532 | . 35998 548086 | 122.449913 | . 35999 548087 | 136.230545 | . 36000 rows × 2 columns . sns.distplot(train_df[&#39;Sales&#39;]) sns.distplot(op_res) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7e3c1e4350&gt; . submission[&#39;Sales&#39;].min() . 25.435012817382812 . (submission[&#39;Sales&#39;]+10).describe() . count 36000.000000 mean 130.931732 std 47.973400 min 35.435013 25% 97.719940 50% 121.766197 75% 154.979839 max 385.928711 Name: Sales, dtype: float64 . submission[&#39;Sales&#39;] = submission[&#39;Sales&#39;] + 10 . submission.to_csv(&#39;results_b10.csv&#39;,index=False) . submission . ID Sales . 0 883 | 115.807068 | . 1 884 | 113.988907 | . 2 885 | 110.990456 | . 3 886 | 103.665955 | . 4 887 | 59.149513 | . ... ... | ... | . 35995 548083 | 160.104187 | . 35996 548084 | 137.545181 | . 35997 548085 | 136.017532 | . 35998 548086 | 132.449921 | . 35999 548087 | 146.230545 | . 36000 rows × 2 columns . Sub analysis . [f for f in os.listdir() if &#39;results&#39; in f] . [&#39;results_exp.csv&#39;, &#39;results_f.csv&#39;, &#39;results_3m.csv&#39;, &#39;results_3m_less.csv&#39;, &#39;results_avg3.csv&#39;] . res_exp = pd.read_csv(&#39;results_exp.csv&#39;) results3m = pd.read_csv(&#39;results_3m.csv&#39;) results3mless = pd.read_csv(&#39;results_3m_less.csv&#39;) . res_exp[&#39;Sales&#39;] = res_exp[&#39;Sales&#39;] + res_exp[&#39;Sales&#39;]*0.1 . res_exp.to_csv(&#39;sales_up.csv&#39;,index=False) . res_exp[&#39;3m&#39;] = results3m[&#39;Sales&#39;] res_exp[&#39;3mless&#39;] = results3mless[&#39;Sales&#39;] . res_exp.describe() . ID Sales 3m 3mless . count 36000.000000 | 36000.000000 | 36000.000000 | 36000.000000 | . mean 274566.035000 | 119.111469 | 122.351114 | 123.795022 | . std 158083.869473 | 47.158998 | 46.292090 | 47.838966 | . min 883.000000 | 20.593432 | 21.874740 | 17.961319 | . 25% 137730.250000 | 86.738166 | 90.401124 | 91.495230 | . 50% 274761.500000 | 110.305010 | 115.157770 | 115.805307 | . 75% 410872.750000 | 141.841580 | 144.824890 | 146.399490 | . max 548087.000000 | 408.644465 | 402.420470 | 411.162140 | . res_exp[&#39;Sales&#39;] = (res_exp[&#39;Sales&#39;]+ res_exp[&#39;3m&#39;]+ res_exp[&#39;3mless&#39;])/3 . res_exp . ID Sales 3m 3mless . 0 883 | 111.272410 | 110.37973 | 106.644400 | . 1 884 | 106.960287 | 107.13604 | 105.499146 | . 2 885 | 104.557614 | 103.99674 | 104.131620 | . 3 886 | 100.815778 | 102.37261 | 99.683880 | . 4 887 | 50.938090 | 53.54695 | 49.258434 | . ... ... | ... | ... | ... | . 35995 548083 | 161.434829 | 164.41560 | 176.355380 | . 35996 548084 | 128.016620 | 132.79832 | 133.331120 | . 35997 548085 | 126.851245 | 132.71388 | 133.144330 | . 35998 548086 | 127.710411 | 133.48079 | 134.173370 | . 35999 548087 | 136.056616 | 143.17442 | 140.773960 | . 36000 rows × 4 columns . res_exp.describe() . ID Sales 3m 3mless . count 36000.000000 | 36000.000000 | 36000.000000 | 36000.000000 | . mean 274566.035000 | 121.752535 | 122.351114 | 123.795022 | . std 158083.869473 | 46.264699 | 46.292090 | 47.838966 | . min 883.000000 | 20.719511 | 21.874740 | 17.961319 | . 25% 137730.250000 | 90.108888 | 90.401124 | 91.495230 | . 50% 274761.500000 | 113.916633 | 115.157770 | 115.805307 | . 75% 410872.750000 | 144.085286 | 144.824890 | 146.399490 | . max 548087.000000 | 407.286963 | 402.420470 | 411.162140 | . res_exp[[&#39;ID&#39;,&#39;Sales&#39;]].to_csv(&#39;results_avg3.csv&#39;,index=False) . 1 . 1 .",
            "url": "http://supriya.org/2020/04/19/av.html",
            "relUrl": "/2020/04/19/av.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Disaster tweets prediction model",
            "content": "import os os.listdir() . [&#39;train.csv&#39;, &#39;tweet_disaster prediction model.ipynb&#39;, &#39;.ipynb_checkpoints&#39;, &#39;models&#39;] . import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns . 12.47 . 12.47 . Data . Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies). Soure:https://www.kaggle.com/c/nlp-getting-started/overview . df = pd.read_csv(&#39;train.csv&#39;) . df.head() . id keyword location text target . 0 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake M... | 1 | . 1 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | . 2 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are ... | 1 | . 3 6 | NaN | NaN | 13,000 people receive #wildfires evacuation or... | 1 | . 4 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as ... | 1 | . df[&#39;target&#39;].value_counts() . 0 4342 1 3271 Name: target, dtype: int64 . sns.barplot(x=[&#39;Normal Tweet&#39;, &#39;Disaster Tweet&#39;], y=df[&#39;target&#39;].value_counts().values) plt.title(&#39;Tweet Distribution&#39;) . Text(0.5, 1.0, &#39;Tweet Distribution&#39;) . Model . from fastai2.text.all import * #from nbdev.showdoc import * . path = Path() path.ls() . (#4) [Path(&#39;train.csv&#39;),Path(&#39;tweet_disaster prediction model.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;models&#39;)] . df.head() . id keyword location text target . 0 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all | 1 | . 1 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | . 2 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are being notified by officers. No other evacuation or shelter in place orders are expected | 1 | . 3 6 | NaN | NaN | 13,000 people receive #wildfires evacuation orders in California | 1 | . 4 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school | 1 | . df.shape . (7613, 5) . Let&#39;s shuffle the data and set 20% of the data for model validation . df = df.sample(frac=1).reset_index(drop=True) . len(df) *0.8 . 6090.400000000001 . df[&#39;is_valid&#39;] = False . df.loc[:int(len(df)*0.8),&#39;is_valid&#39;] = True . df[&#39;is_valid&#39;].value_counts() . True 6091 False 1522 Name: is_valid, dtype: int64 . df[&#39;text&#39;][5000] . &#39;I guess ill never be able to go to mayhem...&#39; . dbunch_lm = TextDataLoaders.from_df(df, text_col=&#39;text&#39;, label_col=&#39;target&#39;, path=path, is_lm=True, valid_col=&#39;is_valid&#39;) . dbunch_lm.train_ds[0] . (TensorText([ 2, 7, 139, 10, 0, 10, 20, 357, 16, 1149, 97, 12, 10, 9, 9, 11, 9, 0, 8, 2757, 10, 7, 292, 7, 1472, 7, 1615, 7, 1313, 55, 2861, 7, 537, 7, 3465, 1726, 35, 35, 7, 82, 27, 54, 8, 0, 44]),) . dbunch_lm.vocab[:20] . [&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;, &#39;xxfld&#39;, &#39;xxrep&#39;, &#39;xxwrep&#39;, &#39;xxup&#39;, &#39;xxmaj&#39;, &#39;/&#39;, &#39;:&#39;, &#39;t.co&#39;, &#39;http&#39;, &#39;#&#39;, &#39;the&#39;, &#39;.&#39;, &#39;a&#39;, &#39;in&#39;, &#39;to&#39;, &#39;of&#39;] . dbunch_lm.show_batch() . text text_ . 0 xxbos xxunk + do anything to fix that . xxmaj of the few people he had every xxunk in his life xxmaj charles was one of the casualties . xxbos xxunk sir i just only wanted to make a point about xxunk you made and said he is lying about bridge collapse . xxbos xxmaj bomb head ? xxmaj xxunk decisions dat produced more dead children than dead bodies trapped xxunk buildings | xxunk + do anything to fix that . xxmaj of the few people he had every xxunk in his life xxmaj charles was one of the casualties . xxbos xxunk sir i just only wanted to make a point about xxunk you made and said he is lying about bridge collapse . xxbos xxmaj bomb head ? xxmaj xxunk decisions dat produced more dead children than dead bodies trapped xxunk buildings on | . 1 / t.co / xxunk m # newsintweets http : / / t.co / xxunk xxbos xxunk dad why do nt you claim me that mean that not right we look the same same eyes same blood same xbox 360 xxup smh xxunk xxbos a xxmaj xxunk of xxmaj two xxmaj xxunk - xxmaj body xxmaj horrors http : / / t.co / xxunk # virus # xxunk # bioterrorism xxbos &#39; if | t.co / xxunk m # newsintweets http : / / t.co / xxunk xxbos xxunk dad why do nt you claim me that mean that not right we look the same same eyes same blood same xbox 360 xxup smh xxunk xxbos a xxmaj xxunk of xxmaj two xxmaj xxunk - xxmaj body xxmaj horrors http : / / t.co / xxunk # virus # xxunk # bioterrorism xxbos &#39; if you | . 2 xxunk xxmaj full ( audio ) xxbos xxunk xxmaj mercury xxmaj cyclone xxup gt xxmaj xxunk xxmaj xxunk xxmaj very xxup nice xxup xxunk xxmaj cobra xxmaj jet xxunk xxup gt http : / / t.co / xxunk http : / / t.co / xxunk xxbos xxmaj there are no four xxunk - of pain of desire that is the origin of pain of the obliteration of that desire of the pain | xxmaj full ( audio ) xxbos xxunk xxmaj mercury xxmaj cyclone xxup gt xxmaj xxunk xxmaj xxunk xxmaj very xxup nice xxup xxunk xxmaj cobra xxmaj jet xxunk xxup gt http : / / t.co / xxunk http : / / t.co / xxunk xxbos xxmaj there are no four xxunk - of pain of desire that is the origin of pain of the obliteration of that desire of the pain to | . 3 : / / t.co / xxunk n▁ # drones # xxunk http : / / t.co / xxunk xxbos xxunk xxunk xxunk xxmaj so … where are the xxunk xxunk and burning buildings xxrep 4 ? xxup white xxup lives xxup matter xxrep 6 ! xxbos xxmaj xxunk xxmaj night and xxmaj xxunk xxmaj xxunk xxrep 3 x enjoy a wild teen xxunk http : / / t.co / qew4c5m1xd xxmaj view | / / t.co / xxunk n▁ # drones # xxunk http : / / t.co / xxunk xxbos xxunk xxunk xxunk xxmaj so … where are the xxunk xxunk and burning buildings xxrep 4 ? xxup white xxup lives xxup matter xxrep 6 ! xxbos xxmaj xxunk xxmaj night and xxmaj xxunk xxmaj xxunk xxrep 3 x enjoy a wild teen xxunk http : / / t.co / qew4c5m1xd xxmaj view and | . 4 # xxmaj manchester on xxmaj rt xxunk xxup nb before xxmaj xxunk xxmaj rd stop and go traffic back to xxup xxunk delay of 4 mins # traffic xxbos xxmaj sinkhole xxunk xxunk opens in housing estate n xxmaj irish xxmaj xxunk xxmaj aug 2015 xxbos # xxmaj bestnaijamade : 16yr old xxup pkk suicide bomber who detonated bomb in … http : / / t.co / ksawlyux02 xxwrep 3 bestnaijamade beû | xxmaj manchester on xxmaj rt xxunk xxup nb before xxmaj xxunk xxmaj rd stop and go traffic back to xxup xxunk delay of 4 mins # traffic xxbos xxmaj sinkhole xxunk xxunk opens in housing estate n xxmaj irish xxmaj xxunk xxmaj aug 2015 xxbos # xxmaj bestnaijamade : 16yr old xxup pkk suicide bomber who detonated bomb in … http : / / t.co / ksawlyux02 xxwrep 3 bestnaijamade beû _ | . 5 xxmaj i &#39;m xxunk the evacuation cost you the front ? xxbos xxmaj cute &amp; &amp; all xxunk &#39; the life then you xxunk in on one &#39;s face and you have a meme ready : &#39; i &#39;ve seen the xxmaj gates of xxmaj hell and survived &#39; xxbos xxmaj xxunk back up xxbos xxmaj yo i got bars and xxmaj i &#39;m not even a xxunk xxbos xxmaj the bomb | i &#39;m xxunk the evacuation cost you the front ? xxbos xxmaj cute &amp; &amp; all xxunk &#39; the life then you xxunk in on one &#39;s face and you have a meme ready : &#39; i &#39;ve seen the xxmaj gates of xxmaj hell and survived &#39; xxbos xxmaj xxunk back up xxbos xxmaj yo i got bars and xxmaj i &#39;m not even a xxunk xxbos xxmaj the bomb was | . 6 xxunk xxup omg xxrep 5 ! xxmaj absolutely xxmaj crazy xxrep 3 ! xxbos xxmaj fears over missing migrants in xxmaj med : xxmaj rescuers search for survivors after a boat carrying as many as 600 xxunk _ http : / / t.co / xxunk xxbos xxup santa xxup cruz ûó xxmaj head of the xxmaj st xxmaj xxunk xxmaj police xxmaj xxunk xxmaj xxunk xxmaj salmon has r … - http | xxup omg xxrep 5 ! xxmaj absolutely xxmaj crazy xxrep 3 ! xxbos xxmaj fears over missing migrants in xxmaj med : xxmaj rescuers search for survivors after a boat carrying as many as 600 xxunk _ http : / / t.co / xxunk xxbos xxup santa xxup cruz ûó xxmaj head of the xxmaj st xxmaj xxunk xxmaj police xxmaj xxunk xxmaj xxunk xxmaj salmon has r … - http : | . 7 xxbos xxunk - symphony of xxmaj destruction http : / / t.co / xxunk xxbos # xxmaj philippines xxmaj former xxmaj township fire truck being used in xxmaj philippines : û _ of emergency equipment in xxmaj xxunk xxmaj xxunk … http : / / t.co / xxunk xxbos a group of xxmaj florida xxmaj forest xxmaj service firefighters could be xxunk to xxmaj california to help xxunk fires . xxmaj details | xxunk - symphony of xxmaj destruction http : / / t.co / xxunk xxbos # xxmaj philippines xxmaj former xxmaj township fire truck being used in xxmaj philippines : û _ of emergency equipment in xxmaj xxunk xxmaj xxunk … http : / / t.co / xxunk xxbos a group of xxmaj florida xxmaj forest xxmaj service firefighters could be xxunk to xxmaj california to help xxunk fires . xxmaj details at | . 8 ! xxup yeah xxup derailed xxup at xxup smithsonian xxup so xxup xxunk xxup is xxup shut xxup down xxup from xxup federal xxup center xxup sw xxup to xxup xxunk xxbos xxmaj if you &#39;re lost and alone or you &#39;re sinking like a stone carry o xxrep 4 n xxbos xxunk by ur . logic if bridge did nt collapse then second train engine should cross bridge then xxunk xxunk | xxup yeah xxup derailed xxup at xxup smithsonian xxup so xxup xxunk xxup is xxup shut xxup down xxup from xxup federal xxup center xxup sw xxup to xxup xxunk xxbos xxmaj if you &#39;re lost and alone or you &#39;re sinking like a stone carry o xxrep 4 n xxbos xxunk by ur . logic if bridge did nt collapse then second train engine should cross bridge then xxunk xxunk xxbos | . Language model . bs = 32 . path.ls() . (#4) [Path(&#39;train.csv&#39;),Path(&#39;tweet_disaster prediction model.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;models&#39;)] . len(dbunch_lm.vocab) . 4568 . learn = language_model_learner(dbunch_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() . learn.lr_find() . SuggestedLRs(lr_min=0.09120108485221863, lr_steep=0.25118863582611084) . learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7,0.8)) . epoch train_loss valid_loss accuracy perplexity time . 0 | 5.255739 | 4.763798 | 0.301351 | 117.190140 | 00:10 | . learn.save(&#39;fit_head&#39;) . learn.load(&#39;fit_head&#39;); . learn.unfreeze() . learn.fit_one_cycle(10, 2e-2, moms=(0.8,0.7,0.8)) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.318294 | 3.830227 | 0.375350 | 46.073002 | 00:10 | . 1 | 4.022458 | 4.364818 | 0.292686 | 78.635094 | 00:09 | . 2 | 3.673906 | 3.652724 | 0.398133 | 38.579617 | 00:09 | . 3 | 3.219373 | 3.670342 | 0.419862 | 39.265324 | 00:09 | . 4 | 2.773297 | 3.853000 | 0.422268 | 47.134274 | 00:09 | . 5 | 2.404130 | 4.008598 | 0.417831 | 55.069626 | 00:09 | . 6 | 2.069800 | 4.024385 | 0.420515 | 55.945915 | 00:09 | . 7 | 1.802565 | 4.064525 | 0.424019 | 58.237221 | 00:09 | . 8 | 1.605108 | 4.040521 | 0.419537 | 56.855938 | 00:09 | . 9 | 1.428457 | 4.034862 | 0.420226 | 56.535095 | 00:10 | . learn.save(&#39;fine_tuned&#39;) . learn.load(&#39;fine_tuned&#39;); . TEXT = &#39;Car accidents&#39; N_WORDS = 30 N_SENTENCES = 2 . print(&quot; n&quot;.join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES))) . Car accidents have drawn no - injuries But I &#39;m happy you &#39;ve survived all of them . Hope you &#39;re okay !!! Do n&#39;t Car accidents have a no - go - go away from the British Bake Back in Kenya http : / / t.co / @worldnetdaily # . learn.save_encoder(&#39;fine_tuned_enc&#39;) . Classifier . tweet_class = DataBlock(blocks=(TextBlock.from_df(&#39;text&#39;,vocab=dbunch_lm.vocab),CategoryBlock), get_y = ColReader(&#39;target&#39;), get_x = ColReader(&#39;text&#39;), splitter=ColSplitter()) . df.head(2) . id keyword location text target is_valid . 0 6102 | hellfire | NaN | @JYHeffect my good you stay in NY??? ? | 0 | True | . 1 8829 | sirens | NaN | @iK4LEN Sirens was cancelled. | 0 | True | . pd.crosstab(df[&#39;is_valid&#39;],df[&#39;target&#39;]) . target 0 1 . is_valid . False 851 | 671 | . True 3491 | 2600 | . dbunch_class = tweet_class.dataloaders(df,path, bs=bs, seq_len=80) . dbunch_class.show_batch() . text category . 0 xxbos xxup info xxup u. xxup xxunk : xxup xxunk xxup xxunk . xxup exp xxup inst xxup apch . xxup rwy 05 . xxup curfew xxup in xxup oper xxup until 2030 xxup z. xxup taxiways xxup foxtrot 5 &amp; &amp; xxup foxtrot 6 xxup navbl . xxup tmp : 10 . xxup wnd : xxunk / 6 . | 0 | . 1 xxbos xxmaj truth … n https : / / t.co / xxunk n▁ # xxmaj news n▁ # xxup bbc n▁ # xxup cnn n▁ # xxmaj islam n▁ # xxmaj truth n▁ # god n▁ # xxup isis n▁ # terrorism n▁ # xxmaj quran n▁ # xxmaj lies http : / / t.co / xxunk | 0 | . 2 xxbos xxmaj learn xxmaj how i xxmaj gained xxmaj access xxmaj to xxmaj the xxmaj secrets xxmaj of xxmaj the xxmaj top xxmaj earners &amp; &amp; xxmaj used xxmaj them xxmaj to xxmaj explode xxmaj my xxmaj home xxmaj business xxmaj here : http : / / t.co / xxunk xxmaj please # xxup rt | 0 | . 3 xxbos xxunk xxup thank xxup you xxup for xxup understanding xxup the xxup gov . xxup only xxup tells xxup us xxup about 5 % xxup of xxup what s xxup really xxup going xxup on i xxup have xxup military xxup house &amp; &amp; xxup cia xxup xxunk xxrep 3 ! | 0 | . 4 xxbos xxmaj xxunk xxmaj hot xxmaj deals # xxunk &gt; &gt; http : / / t.co / xxunk xxunk xxunk xxunk xxunk xxup led xxmaj work xxmaj light xxup flood xxmaj lamp xxmaj xxunk xxmaj truck xxup suv xxup xxunk xxup aû _ http : / / t.co / xxunk | 0 | . 5 xxbos # nowplaying * xxmaj cliff xxmaj richard - i xxmaj could xxmaj easily xxmaj fall ( in xxmaj love xxmaj with xxmaj you ) ( &amp; &amp; xxmaj xxunk ) * # xxmaj internet # xxmaj xxunk # xxmaj radio xxmaj on http : / / t.co / xxunk | 0 | . 6 xxbos xxup i &#39;m xxup laughing xxup in xxup the xxup face xxup of xxup casualties xxup and xxup xxunk xxup the xxup first xxup time xxup i &#39;m xxup thinking xxup past xxup tomorrow xxup but i xxup am xxup not xxup xxunk xxup away xxup my xxup shot | 1 | . 7 xxbos xxmaj haha xxmaj south xxmaj tampa is getting flooded xxunk xxup wait a xxup second i xxup live xxup in xxup south xxup tampa xxup what xxup am i xxup gon na xxup do xxup what xxup am i xxup gon na xxup do xxup xxunk # flooding | 1 | . 8 xxbos xxmaj conditions for xxmaj paris xxup fr at xxunk am xxup xxunk : xxmaj current xxmaj conditions : n xxmaj fair xxunk xxunk : n xxmaj xxunk - xxmaj xxunk . xxmaj high : xxunk xxmaj low : xxunk n xxmaj xxunk - xxup pm xxmaj thunderstorm … | 0 | . learn = text_classifier_learner(dbunch_class, AWD_LSTM, drop_mult=0.3, metrics=accuracy).to_fp16() learn.load_encoder(&#39;fine_tuned_enc&#39;) . &lt;fastai2.text.learner.TextLearner at 0x7f89115c43d0&gt; . learn.lr_find() . SuggestedLRs(lr_min=0.02089296132326126, lr_steep=0.004365158267319202) . learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7, 0.8)) . epoch train_loss valid_loss accuracy time . 0 | 0.765621 | 0.529380 | 0.748481 | 00:09 | . learn.save(&#39;first&#39;) learn.load(&#39;first&#39;) . &lt;fastai2.text.learner.TextLearner at 0x7f89115c43d0&gt; . learn.freeze_to(-2) learn.fit_one_cycle(5, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7, 0.8)) . epoch train_loss valid_loss accuracy time . 0 | 0.658757 | 0.565911 | 0.727959 | 00:10 | . 1 | 0.541278 | 0.629307 | 0.746183 | 00:08 | . 2 | 0.452028 | 0.625421 | 0.756690 | 00:05 | . 3 | 0.381687 | 0.638110 | 0.734198 | 00:05 | . 4 | 0.312132 | 0.645469 | 0.738302 | 00:05 | . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3),moms=(0.8,0.7, 0.8)) . epoch train_loss valid_loss accuracy time . 0 | 0.229639 | 0.703110 | 0.737810 | 00:05 | . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7, 0.8)) . epoch train_loss valid_loss accuracy time . 0 | 0.225316 | 0.719005 | 0.736332 | 00:06 | . 1 | 0.195596 | 0.736392 | 0.741586 | 00:06 | . learn.fit_one_cycle(4,slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7, 0.8)) . epoch train_loss valid_loss accuracy time . 0 | 0.196479 | 0.749885 | 0.741586 | 00:06 | . 1 | 0.171783 | 0.801530 | 0.732556 | 00:06 | . 2 | 0.153960 | 0.826557 | 0.740437 | 00:06 | . 3 | 0.147267 | 0.789741 | 0.740273 | 00:10 | . learn.predict(&quot;there&#39;s a fire emergency in my city&quot;) . (&#39;1&#39;, tensor(1), tensor([2.0377e-07, 1.0000e+00])) . learn.predict(&quot;What a peaceful place to live&quot;) . (&#39;1&#39;, tensor(1), tensor([0.0318, 0.9682])) . learn.show_results() . text category category_ . 0 xxbos _ n▁ xxrep 5 ? xxup retweet n▁ xxrep 7 ? n▁ xxrep 5 ? xxup follow xxup all xxup who xxup rt n▁ xxrep 7 ? n▁ xxrep 5 ? xxup xxunk n▁ xxrep 7 ? n▁ xxrep 5 ? xxup gain xxup with n▁ xxrep 7 ? n▁ xxrep 5 ? xxup follow ? xxunk # xxup xxunk n▁ # xxup ty | 0 | 0 | . 1 xxbos xxmaj truth … n https : / / t.co / xxunk n▁ # xxmaj news n▁ # xxup bbc n▁ # xxup cnn n▁ # xxmaj islam n▁ # xxmaj truth n▁ # god n▁ # xxup isis n▁ # terrorism n▁ # xxmaj quran n▁ # xxmaj lies http : / / t.co / xxunk | 0 | 0 | . 2 xxbos xxmaj truth … n https : / / t.co / xxunk n▁ # xxmaj news n▁ # xxup bbc n▁ # xxup cnn n▁ # xxmaj islam n▁ # xxmaj truth n▁ # god n▁ # xxup isis n▁ # terrorism n▁ # xxmaj quran n▁ # xxmaj lies http : / / t.co / xxunk | 1 | 0 | . 3 xxbos xxmaj truth … n https : / / t.co / xxunk n▁ # xxmaj news n▁ # xxup bbc n▁ # xxup cnn n▁ # xxmaj islam n▁ # xxmaj truth n▁ # god n▁ # xxup isis n▁ # terrorism n▁ # xxmaj quran n▁ # xxmaj lies http : / / t.co / xxunk | 1 | 0 | . 4 xxbos xxmaj truth … n https : / / t.co / xxunk n▁ # xxmaj news n▁ # xxup bbc n▁ # xxup cnn n▁ # xxmaj islam n▁ # xxmaj truth n▁ # god n▁ # xxup isis n▁ # terrorism n▁ # xxmaj quran n▁ # xxmaj lies http : / / t.co / xxunk | 1 | 0 | . 5 xxbos xxmaj truth … n https : / / t.co / xxunk n▁ # xxmaj news n▁ # xxup bbc n▁ # xxup cnn n▁ # xxmaj islam n▁ # xxmaj truth n▁ # god n▁ # xxup isis n▁ # terrorism n▁ # xxmaj quran n▁ # xxmaj lies http : / / t.co / xxunk | 0 | 0 | . 6 xxbos xxmaj learn xxmaj how i xxmaj gained xxmaj access xxmaj to xxmaj the xxmaj secrets xxmaj of xxmaj the xxmaj top xxmaj earners &amp; &amp; xxmaj used xxmaj them xxmaj to xxmaj explode xxmaj my xxmaj home xxmaj business xxmaj here : http : / / t.co / xxup xxunk xxmaj please # xxup rt | 0 | 0 | . 7 xxbos xxmaj learn xxmaj how i xxmaj gained xxmaj access xxmaj to xxmaj the xxmaj secrets xxmaj of xxmaj the xxmaj top xxmaj earners &amp; &amp; xxmaj used xxmaj them xxmaj to xxmaj explode xxmaj my xxmaj home xxmaj business xxmaj here : http : / / t.co / xxunk xxmaj please # xxup rt | 0 | 0 | . 8 xxbos xxmaj morgan xxmaj silver xxmaj dollar xxunk s xxmaj gem xxup bu xxup xxunk xxmaj xxunk xxmaj xxunk xxmaj blazing xxup ms xxrep 5 + xxmaj high grade ! - xxmaj full read û _ http : / / t.co / xxunk http : / / t.co / xxunk | 0 | 0 | . Interpretation . from fastai.text.interpret import * . interp = ClassificationInterpretation.from_learner(learn) . interp.confusion_matrix() . array([[2783, 708], [ 874, 1726]]) . interp.plot_top_losses(5) . input target predicted probability loss . 0 xxbos xxmaj hellfire is surrounded by desires so be careful and donûªt let your desires control you ! # xxmaj afterlife | 1 | 0 | 0.9999980926513672 | 13.148439407348633 | . 1 xxbos xxmaj hellfire ! xxmaj we donûªt even want to think about it or mention it so letûªs not do anything that leads to it # islam ! | 1 | 0 | 0.9999979734420776 | 13.109376907348633 | . 2 xxbos xxmaj large rain xxunk falling in xxmaj rock xxmaj hill off xxmaj xxunk xxmaj road . # rain # xxunk # drought | 0 | 1 | 0.9999972581863403 | 12.82422161102295 | . 3 xxbos xxmaj xxunk police investigating pedestrian fatality hit by a train xxmaj thursday http : / / t.co / xxunk | 0 | 1 | 0.9999963045120239 | 12.523441314697266 | . 4 xxbos xxmaj even if u have your weapon and your xxunk we gon na put them xxunk on your ass xxrep 4 ? | 1 | 0 | 0.9999911785125732 | 11.640633583068848 | . interp.plot_confusion_matrix() .",
            "url": "http://supriya.org/2020/04/17/tweet.html",
            "relUrl": "/2020/04/17/tweet.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Covid",
            "content": "import os os.listdir() . [&#39;Covid_Experiments.ipynb&#39;, &#39;Covid_Production.ipynb&#39;, &#39;Untitled.ipynb&#39;, &#39;covid.jpeg&#39;, &#39;Covid_Production.html&#39;, &#39;Untitled.html&#39;, &#39;data&#39;, &#39;data.zip&#39;, &#39;Untitled1.html&#39;, &#39;Covid_Experiments.html&#39;, &#39;.ipynb_checkpoints&#39;, &#39;Untitled1.ipynb&#39;, &#39;__MACOSX&#39;, &#39;blood_cells&#39;] . #! unzip data.zip . path = Path(&#39;data&#39;) . path_img = path/&#39;data/&#39; . path.ls() . (#3) [Path(&#39;data/covid&#39;),Path(&#39;data/normal&#39;),Path(&#39;data/pneumonia&#39;)] . #path.ls()[0].unlink() . path.ls() . (#3) [Path(&#39;data/covid&#39;),Path(&#39;data/normal&#39;),Path(&#39;data/pneumonia&#39;)] . fnames = get_image_files(path) . fnames . (#956) [Path(&#39;data/covid/wong-0005.jpg&#39;),Path(&#39;data/covid/fff49165-b22d-4bb4-b9d1-d5d62c52436c.annot.original.png&#39;),Path(&#39;data/covid/446B2CB6-B572-40AB-B01F-1910CA07086A.jpeg&#39;),Path(&#39;data/covid/pneumocystis-pneumonia-2-L.png&#39;),Path(&#39;data/covid/3ED3C0E1-4FE0-4238-8112-DDFF9E20B471.jpeg&#39;),Path(&#39;data/covid/ryct.2020200028.fig1a.jpeg&#39;),Path(&#39;data/covid/auntminnie-d-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg&#39;),Path(&#39;data/covid/covid-19-pneumonia-22-day1-pa.png&#39;),Path(&#39;data/covid/covid-19-pneumonia-bilateral.jpg&#39;),Path(&#39;data/covid/44C8E3D6-20DA-42E9-B33B-96FA6D6DE12F.jpeg&#39;)...] . data = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.3,seed=2), get_y=parent_label, item_tfms=Resize(224) ) . dls = data.dataloaders(path) . dls.valid.show_batch(max_n=4, nrows=1) . tfms = partial(aug_transforms, max_rotate=10, max_zoom=1.3, max_lighting=0.4, max_warp=0.4, p_affine=1., p_lighting=1.) . def get_dls(size, bs): covid = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(0.2, seed=2), get_y=parent_label, item_tfms=RandomResizedCrop(400, min_scale=0.75), batch_tfms=[*tfms(size=size,do_flip=False), Normalize.from_stats(*imagenet_stats)]) return covid.dataloaders(path, path=path, bs=bs) . dls = get_dls(224,64) . dls.valid.show_batch(max_n=9, nrows=3) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.481249 | 1.383973 | 0.366492 | 00:14 | . epoch train_loss valid_loss error_rate time . 0 | 0.549677 | 0.770690 | 0.272251 | 00:14 | . 1 | 0.419305 | 0.409678 | 0.130890 | 00:14 | . 2 | 0.355615 | 0.187622 | 0.041885 | 00:14 | . 3 | 0.312927 | 0.219824 | 0.047120 | 00:15 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . # learn.fine_tune(4) . # interp = ClassificationInterpretation.from_learner(learn) # interp.plot_confusion_matrix() . interp.plot_top_losses(3,nrows=1,figsize=(15,5)) . learn.unfreeze() . learn.lr_find() . SuggestedLRs(lr_min=2.290867705596611e-05, lr_steep=7.585775847473997e-07) . learn.fit_one_cycle(10,1e-4) . epoch train_loss valid_loss error_rate time . 0 | 0.163507 | 0.229232 | 0.036649 | 00:14 | . 1 | 0.196887 | 0.271420 | 0.036649 | 00:15 | . 2 | 0.195378 | 0.198634 | 0.041885 | 00:15 | . 3 | 0.190075 | 0.201389 | 0.026178 | 00:15 | . 4 | 0.171067 | 0.237981 | 0.031414 | 00:14 | . 5 | 0.168759 | 0.188887 | 0.015707 | 00:13 | . 6 | 0.153896 | 0.143375 | 0.015707 | 00:14 | . 7 | 0.147037 | 0.169176 | 0.015707 | 00:13 | . 8 | 0.131743 | 0.164342 | 0.015707 | 00:13 | . 9 | 0.118339 | 0.169921 | 0.015707 | 00:13 | . Model Results . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(3,nrows=1,figsize=(15,5)) . interp.print_classification_report() . precision recall f1-score support covid 1.00 1.00 1.00 61 normal 0.95 1.00 0.98 59 pneumonia 1.00 0.96 0.98 71 accuracy 0.98 191 macro avg 0.98 0.99 0.98 191 weighted avg 0.99 0.98 0.98 191 . Application . learn.export() . path = Path(&#39;data&#39;) path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;data/export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . path.ls() . (#4) [Path(&#39;data/pneumonia&#39;),Path(&#39;data/normal&#39;),Path(&#39;data/covid&#39;),Path(&#39;data/export.pkl&#39;)] . learn_inf.predict(&#39;data/pneumonia/person72_bacteria_353.jpeg&#39;) . (&#39;pneumonia&#39;, tensor(2), tensor([3.4933e-04, 8.0628e-04, 9.9884e-01])) . learn_inf.dls.vocab . (#3) [&#39;covid&#39;,&#39;normal&#39;,&#39;pneumonia&#39;] . Notebook app . btn_upload = widgets.FileUpload() btn_upload . img = PILImage.create(btn_upload.data[-1]) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(224,224)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . HBox([widgets.Label(&#39;Please upload your X-ray&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . # #hide # !pip install voila # !jupyter serverextension enable voila --sys-prefix . Heatmap . idx=50 x,y = dls.valid_ds[idx] show_at(dls.valid_ds, idx); . m = learn.model.eval(); . b = dls.one_batch() xb_im = TensorImage(dls.train.decode(b)[0][0]) xb = b[0] . def hooked_backward(cat=y): with hook_output(m[0]) as hook_a: with hook_output(m[0], grad=True) as hook_g: preds = m(xb) preds[0,int(cat)].backward() return hook_a,hook_g . hook_a,hook_g = hooked_backward() . acts = hook_a.stored[0].cpu() acts.shape . torch.Size([512, 7, 7]) . avg_acts = acts.mean(0) avg_acts.shape . torch.Size([7, 7]) . def show_heatmap(hm): _,ax = plt.subplots() xb_im.show(ctx=ax) ax.imshow(hm, alpha=0.5, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . show_heatmap(avg_acts) . avg_acts . tensor([[0.3327, 0.6411, 1.0377, 0.9832, 0.8696, 0.5856, 0.4433], [0.3971, 0.9722, 1.5335, 1.4393, 1.1379, 0.7690, 0.5907], [0.4813, 1.0672, 1.4148, 1.3892, 1.0483, 0.9821, 0.8356], [0.5228, 1.0640, 1.2774, 1.3052, 1.2049, 1.4229, 1.2645], [0.6588, 0.9745, 1.1926, 1.3003, 1.4558, 1.8145, 1.5375], [0.7019, 1.0060, 1.2029, 1.2723, 1.4817, 1.7945, 1.4456], [0.5527, 0.8330, 0.9566, 0.9694, 1.0395, 1.3031, 1.0919]]) . Grad-CAM . grad = hook_g.stored[0][0].cpu() grad_chan = grad.mean(1).mean(1) grad.shape,grad_chan.shape . (torch.Size([512, 7, 7]), torch.Size([512])) . mult = (acts*grad_chan[...,None,None]).mean(0) . show_heatmap(mult) . fnames . (#956) [Path(&#39;data/pneumonia/person63_bacteria_306.jpeg&#39;),Path(&#39;data/pneumonia/person26_bacteria_122.jpeg&#39;),Path(&#39;data/pneumonia/person69_bacteria_338.jpeg&#39;),Path(&#39;data/pneumonia/person60_bacteria_287.jpeg&#39;),Path(&#39;data/pneumonia/person82_virus_154.jpeg&#39;),Path(&#39;data/pneumonia/person53_bacteria_254.jpeg&#39;),Path(&#39;data/pneumonia/person74_bacteria_362.jpeg&#39;),Path(&#39;data/pneumonia/person55_bacteria_265.jpeg&#39;),Path(&#39;data/pneumonia/person72_bacteria_353.jpeg&#39;),Path(&#39;data/pneumonia/person64_bacteria_316.jpeg&#39;)...] . [c for c in fnames if &#39;covid&#39; in str(c)] . [Path(&#39;data/covid/radiopaedia-2019-novel-coronavirus-infected-pneumonia.jpg&#39;), Path(&#39;data/covid/9fdd3c3032296fd04d2cad5d9070d4_jumbo.jpeg&#39;), Path(&#39;data/covid/radiol.2020201160.fig6b.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-rapidly-progressive-3-days.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-23-day9.jpg&#39;), Path(&#39;data/covid/pneumocystis-jirovecii-pneumonia-3-3.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-22-day2-pa.png&#39;), Path(&#39;data/covid/radiol.2020201160.fig2c.jpeg&#39;), Path(&#39;data/covid/396A81A5-982C-44E9-A57E-9B1DC34E2C08.jpeg&#39;), Path(&#39;data/covid/8FDE8DBA-CFBD-4B4C-B1A4-6F36A93B7E87.jpeg&#39;), Path(&#39;data/covid/jkms-35-e79-g001-l-e.jpg&#39;), Path(&#39;data/covid/jkms-35-e79-g001-l-d.jpg&#39;), Path(&#39;data/covid/figure1-5e7c1b8d98c29ab001275405-98.jpeg&#39;), Path(&#39;data/covid/pneumocystis-jirovecii-pneumonia-3-2.jpg&#39;), Path(&#39;data/covid/da9e9aac-de8c-44c7-ba57-e7cc8e4caaba.annot.original.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-58-prior.jpg&#39;), Path(&#39;data/covid/31BA3780-2323-493F-8AED-62081B9C383B.jpeg&#39;), Path(&#39;data/covid/7C69C012-7479-493F-8722-ABC29C60A2DD.jpeg&#39;), Path(&#39;data/covid/pneumocystis-carinii-pneumonia-1-PA.jpg&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g05x-Fig5-day9.jpeg&#39;), Path(&#39;data/covid/1-s2.0-S0929664620300449-gr3_lrg-d.jpg&#39;), Path(&#39;data/covid/44C8E3D6-20DA-42E9-B33B-96FA6D6DE12F.jpeg&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g09a-Fig9a-day17.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-rapidly-progressive-12-hours.jpg&#39;), Path(&#39;data/covid/446B2CB6-B572-40AB-B01F-1910CA07086A.jpeg&#39;), Path(&#39;data/covid/nejmoa2001191_f3-L.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-7-PA.jpg&#39;), Path(&#39;data/covid/pneumocystis-pneumonia-2-L.png&#39;), Path(&#39;data/covid/pneumocystis-pneumonia-12.png&#39;), Path(&#39;data/covid/pneumocystis-jirovecii-pneumonia-3-1.jpg&#39;), Path(&#39;data/covid/figure1-5e73d7ae897e27ff066a30cb-98.jpeg&#39;), Path(&#39;data/covid/nejmoa2001191_f5-PA.jpeg&#39;), Path(&#39;data/covid/pneumocystis-pneumonia-1.jpg&#39;), Path(&#39;data/covid/ARDSSevere.png&#39;), Path(&#39;data/covid/aspiration-pneumonia-5-day27.jpg&#39;), Path(&#39;data/covid/right-upper-lobe-pneumonia-9-L.jpg&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g0-Fig8a-day0.jpeg&#39;), Path(&#39;data/covid/1-s2.0-S0929664620300449-gr3_lrg-a.jpg&#39;), Path(&#39;data/covid/pneumocystis-carinii-pneumonia-1-L.jpg&#39;), Path(&#39;data/covid/nejmoa2001191_f1-L.jpeg&#39;), Path(&#39;data/covid/21DDEBFD-7F16-4E3E-8F90-CB1B8EE82828.jpeg&#39;), Path(&#39;data/covid/nejmc2001573_f1a.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-43-day0.jpeg&#39;), Path(&#39;data/covid/streptococcus-pneumoniae-pneumonia-1.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-30-PA.jpg&#39;), Path(&#39;data/covid/cavitating-pneumonia-4-day28-L.png&#39;), Path(&#39;data/covid/23E99E2E-447C-46E5-8EB2-D35D12473C39.png&#39;), Path(&#39;data/covid/jkms-35-e79-g001-l-c.jpg&#39;), Path(&#39;data/covid/jkms-35-e79-g001-l-b.jpg&#39;), Path(&#39;data/covid/2B8649B2-00C4-4233-85D5-1CE240CF233B.jpeg&#39;), Path(&#39;data/covid/aspiration-pneumonia-5-day0.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-14-PA.png&#39;), Path(&#39;data/covid/auntminnie-a-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg&#39;), Path(&#39;data/covid/1-s2.0-S0929664620300449-gr3_lrg-b.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-44-day-0.jpeg&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g04a-Fig4a-day7.jpeg&#39;), Path(&#39;data/covid/F63AB6CE-1968-4154-A70F-913AF154F53D.jpeg&#39;), Path(&#39;data/covid/cavitating-pneumonia-4-day0-L.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-38-l.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-49-day8.jpg&#39;), Path(&#39;data/covid/jkms-35-e79-g001-l-a.jpg&#39;), Path(&#39;data/covid/radiol.2020201160.fig2b.jpeg&#39;), Path(&#39;data/covid/radiol.2020201160.fig3d.jpeg&#39;), Path(&#39;data/covid/aspiration-pneumonia-5-day3.jpg&#39;), Path(&#39;data/covid/1-s2.0-S0929664620300449-gr3_lrg-c.jpg&#39;), Path(&#39;data/covid/radiol.2020200274.fig3d.png&#39;), Path(&#39;data/covid/covid-19-pneumonia-evolution-over-a-week-1-day0-L.jpg&#39;), Path(&#39;data/covid/D5ACAA93-C779-4E22-ADFA-6A220489F840.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-evolution-over-a-week-1-day4.jpg&#39;), Path(&#39;data/covid/4C4DEFD8-F55D-4588-AAD6-C59017F55966.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-35-1.jpg&#39;), Path(&#39;data/covid/5e6dd879fde9502400e58b2f.jpeg&#39;), Path(&#39;data/covid/covid-19-rapidly-progressive-acute-respiratory-distress-syndrome-ards-day-2.jpg&#39;), Path(&#39;data/covid/5083A6B7-8983-472E-A427-570A3E03DDEE.jpeg&#39;), Path(&#39;data/covid/7E335538-2F86-424E-A0AB-6397783A38D0.jpeg&#39;), Path(&#39;data/covid/nCoV-Snohomish-20382862_web1_M1-Lungs-EDH-200201-640x300@2x.jpg&#39;), Path(&#39;data/covid/ryct.2020200034.fig5-day4.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-58-day-10.jpg&#39;), Path(&#39;data/covid/4e43e48d52c9e2d4c6c1fb9bc1544f_jumbo.jpeg&#39;), Path(&#39;data/covid/covid-19-rapidly-progressive-acute-respiratory-distress-syndrome-ards-day-3.jpg&#39;), Path(&#39;data/covid/pneumococcal-pneumonia-day0.jpg&#39;), Path(&#39;data/covid/lancet-case2a.jpg&#39;), Path(&#39;data/covid/5CBC2E94-D358-401E-8928-965CCD965C5C.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-38-pa.jpg&#39;), Path(&#39;data/covid/nejmoa2001191_f5-L.jpeg&#39;), Path(&#39;data/covid/66298CBF-6F10-42D5-A688-741F6AC84A76.jpeg&#39;), Path(&#39;data/covid/nCoV-radiol.2020200269.fig1-day7.jpeg&#39;), Path(&#39;data/covid/pneumocystis-jiroveci-pneumonia-4-PA.png&#39;), Path(&#39;data/covid/covid-19-pneumonia-7-L.jpg&#39;), Path(&#39;data/covid/figure1-5e75d0940b71e1b702629659-98-right.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-35-2.jpg&#39;), Path(&#39;data/covid/MERS-CoV-1-s2.0-S0378603X1500248X-gr4e.jpg&#39;), Path(&#39;data/covid/radiol.2020201160.fig3c.jpeg&#39;), Path(&#39;data/covid/1-s2.0-S0929664620300449-gr2_lrg-d.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-20-pa-on-admission.jpg&#39;), Path(&#39;data/covid/1B734A89-A1BF-49A8-A1D3-66FAFA4FAC5D.jpeg&#39;), Path(&#39;data/covid/covid-19-rapidly-progressive-acute-respiratory-distress-syndrome-ards-day-1.jpg&#39;), Path(&#39;data/covid/6A7D4110-2BFC-4D9A-A2D6-E9226D91D25A.jpeg&#39;), Path(&#39;data/covid/RX-torace-a-letto-del-paziente-in-unica-proiezione-AP-1-1.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-28.png&#39;), Path(&#39;data/covid/covid-19-pneumonia-evolution-over-a-week-1-day0-PA.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-58-day-7.jpg&#39;), Path(&#39;data/covid/lancet-case2b.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-evolution-over-a-week-1-day6.jpg&#39;), Path(&#39;data/covid/auntminnie-c-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg&#39;), Path(&#39;data/covid/03BF7561-A9BA-4C3C-B8A0-D3E585F73F3C.jpeg&#39;), Path(&#39;data/covid/cavitating-pneumonia-4-day0-PA.jpg&#39;), Path(&#39;data/covid/legionella-pneumonia-1.png&#39;), Path(&#39;data/covid/ryct.2020003.fig2-b.png&#39;), Path(&#39;data/covid/covid-19-pneumonia-41-day-0.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-58-day-3.jpg&#39;), Path(&#39;data/covid/1-s2.0-S0929664620300449-gr2_lrg-a.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-rapidly-progressive-admission.jpg&#39;), Path(&#39;data/covid/kjr-21-e24-g004-l-b.jpg&#39;), Path(&#39;data/covid/pneumococcal-pneumonia-day7.jpg&#39;), Path(&#39;data/covid/1.CXRCTThoraximagesofCOVID-19fromSingapore.pdf-001-fig2b.png&#39;), Path(&#39;data/covid/D7AF463C-2369-492D-908D-BE1911CCD74C.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-2.jpg&#39;), Path(&#39;data/covid/4ad30bc6-2da0-4f84-bc9b-62acabfd518a.annot.original.png&#39;), Path(&#39;data/covid/auntminnie-b-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg&#39;), Path(&#39;data/covid/chlamydia-pneumonia-PA.png&#39;), Path(&#39;data/covid/radiol.2020201160.fig2d.jpeg&#39;), Path(&#39;data/covid/covid-19-infection-exclusive-gastrointestinal-symptoms-l.png&#39;), Path(&#39;data/covid/radiol.2020201160.fig3b.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-evolution-over-a-week-1-day3.jpg&#39;), Path(&#39;data/covid/ryct.2020003.fig2-c.png&#39;), Path(&#39;data/covid/gr1_lrg-a.jpg&#39;), Path(&#39;data/covid/pneumocystis-jiroveci-pneumonia-4-L.png&#39;), Path(&#39;data/covid/1F6343EE-AFEC-4B7D-97F5-62797EE18767.jpeg&#39;), Path(&#39;data/covid/ryct.2020003.fig2-a.png&#39;), Path(&#39;data/covid/legionella-pneumonia-2.jpg&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g0-Fig8b-day5.jpeg&#39;), Path(&#39;data/covid/kjr-21-e24-g004-l-a.jpg&#39;), Path(&#39;data/covid/1-s2.0-S0929664620300449-gr2_lrg-b.jpg&#39;), Path(&#39;data/covid/1.CXRCTThoraximagesofCOVID-19fromSingapore.pdf-001-fig2a.png&#39;), Path(&#39;data/covid/auntminnie-2020_01_31_20_24_2322_2020_01_31_x-ray_coronavirus_US.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-12.jpg&#39;), Path(&#39;data/covid/klebsiella-pneumonia-1.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-67.jpeg&#39;), Path(&#39;data/covid/1-s2.0-S0929664620300449-gr2_lrg-c.jpg&#39;), Path(&#39;data/covid/E63574A7-4188-4C8D-8D17-9D67A18A1AFA.jpeg&#39;), Path(&#39;data/covid/radiol.2020200490.fig3.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-41-day-2.jpg&#39;), Path(&#39;data/covid/gr1_lrg-b.jpg&#39;), Path(&#39;data/covid/ryct.2020200028.fig1a.jpeg&#39;), Path(&#39;data/covid/post-intubuation-pneumomediastium-and-pneumothorax-background-covid-19-pneumonia-day7.jpg&#39;), Path(&#39;data/covid/1-s2.0-S1684118220300682-main.pdf-002-a1.png&#39;), Path(&#39;data/covid/ciaa199.pdf-001-c.png&#39;), Path(&#39;data/covid/covid-19-pneumonia-24-day6.jpg&#39;), Path(&#39;data/covid/1.CXRCTThoraximagesofCOVID-19fromSingapore.pdf-002-fig3a.png&#39;), Path(&#39;data/covid/covid-19-pneumonia-24-day7.jpg&#39;), Path(&#39;data/covid/A7E260CE-8A00-4C5F-A7F5-27336527A981.jpeg&#39;), Path(&#39;data/covid/figure1-5e7c1b8d98c29ab001275405-98-later.jpeg&#39;), Path(&#39;data/covid/ciaa199.pdf-001-b.png&#39;), Path(&#39;data/covid/6CB4EFC6-68FA-4CD5-940C-BEFA8DAFE9A7.jpeg&#39;), Path(&#39;data/covid/kjr-21-e24-g001-l-a.jpg&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g09c-Fig9c-day27.jpeg&#39;), Path(&#39;data/covid/41591_2020_819_Fig1_HTML.webp-day10.png&#39;), Path(&#39;data/covid/parapneumonic-effusion-1-L.png&#39;), Path(&#39;data/covid/9C34AF49-E589-44D5-92D3-168B3B04E4A6.jpeg&#39;), Path(&#39;data/covid/kjr-21-e24-g001-l-c.jpg&#39;), Path(&#39;data/covid/1-s2.0-S1684118220300682-main.pdf-002-a2.png&#39;), Path(&#39;data/covid/3ED3C0E1-4FE0-4238-8112-DDFF9E20B471.jpeg&#39;), Path(&#39;data/covid/1-s2.0-S0140673620303706-fx1_lrg.jpg&#39;), Path(&#39;data/covid/1.CXRCTThoraximagesofCOVID-19fromSingapore.pdf-002-fig3b.png&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g0-Fig8c-day10.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-34.png&#39;), Path(&#39;data/covid/53EC07C9-5CC6-4BE4-9B6F-D7B0D72AAA7E.jpeg&#39;), Path(&#39;data/covid/ciaa199.pdf-001-a.png&#39;), Path(&#39;data/covid/pneumonia-7.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-20.jpg&#39;), Path(&#39;data/covid/5A78BCA9-5B7A-440D-8A4E-AE7710EA6EAD.jpeg&#39;), Path(&#39;data/covid/radiol.2020201160.fig3a.jpeg&#39;), Path(&#39;data/covid/kjr-21-e24-g001-l-b.jpg&#39;), Path(&#39;data/covid/254B82FC-817D-4E2F-AB6E-1351341F0E38.jpeg&#39;), Path(&#39;data/covid/covid-19-infection-exclusive-gastrointestinal-symptoms-pa.png&#39;), Path(&#39;data/covid/7D2CF6CE-F529-4470-8356-D33FFAF98600.jpeg&#39;), Path(&#39;data/covid/post-intubuation-pneumomediastium-and-pneumothorax-background-covid-19-pneumonia-day6-1.jpg&#39;), Path(&#39;data/covid/post-intubuation-pneumomediastium-and-pneumothorax-background-covid-19-pneumonia-day1.jpg&#39;), Path(&#39;data/covid/chlamydia-pneumonia-L.png&#39;), Path(&#39;data/covid/1312A392-67A3-4EBF-9319-810CF6DA5EF6.jpeg&#39;), Path(&#39;data/covid/streptococcus-pneumoniae-pneumonia-temporal-evolution-1-day1.jpg&#39;), Path(&#39;data/covid/01E392EE-69F9-4E33-BFCE-E5C968654078.jpeg&#39;), Path(&#39;data/covid/93FE0BB1-022D-4F24-9727-987A07975FFB.jpeg&#39;), Path(&#39;data/covid/kjr-21-e25-g001-l-a.jpg&#39;), Path(&#39;data/covid/6C94A287-C059-46A0-8600-AFB95F4727B7.jpeg&#39;), Path(&#39;data/covid/acute-respiratory-distress-syndrome-ards.jpg&#39;), Path(&#39;data/covid/right-upper-lobe-pneumonia-9-PA.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-22-day1-pa.png&#39;), Path(&#39;data/covid/acute-respiratory-distress-syndrome-ards-1.jpg&#39;), Path(&#39;data/covid/covid-19-caso-70-1-L.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-19.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-15-L.jpg&#39;), Path(&#39;data/covid/2966893D-5DDF-4B68-9E2B-4979D5956C8E.jpeg&#39;), Path(&#39;data/covid/pneumocystis-jiroveci-pneumonia-2.png&#39;), Path(&#39;data/covid/1.CXRCTThoraximagesofCOVID-19fromSingapore.pdf-003-fig4a.png&#39;), Path(&#39;data/covid/streptococcus-pneumoniae-pneumonia-temporal-evolution-1-day0.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-bilateral.jpg&#39;), Path(&#39;data/covid/FE9F9A5D-2830-46F9-851B-1FF4534959BE.jpeg&#39;), Path(&#39;data/covid/pneumocystis-jirovecii-pneumonia-2.jpg&#39;), Path(&#39;data/covid/E1724330-1866-4581-8CD8-CEC9B8AFEDDE.jpeg&#39;), Path(&#39;data/covid/post-intubuation-pneumomediastium-and-pneumothorax-background-covid-19-pneumonia-day6-2.jpg&#39;), Path(&#39;data/covid/streptococcus-pneumoniae-pneumonia-temporal-evolution-1-day2.jpg&#39;), Path(&#39;data/covid/nejmoa2001191_f3-PA.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-58-day-9.jpg&#39;), Path(&#39;data/covid/all14238-fig-0001-m-b.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-15-PA.jpg&#39;), Path(&#39;data/covid/ryct.2020200034.fig5-day7.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-8.jpg&#39;), Path(&#39;data/covid/191F3B3A-2879-4EF3-BE56-EE0D2B5AAEE3.jpeg&#39;), Path(&#39;data/covid/all14238-fig-0001-m-c.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-44-day-8.jpeg&#39;), Path(&#39;data/covid/1.CXRCTThoraximagesofCOVID-19fromSingapore.pdf-003-fig4b.png&#39;), Path(&#39;data/covid/streptococcus-pneumoniae-pneumonia-temporal-evolution-1-day3.jpg&#39;), Path(&#39;data/covid/353889E0-A1E8-4F9E-A0B8-F24F36BCFBFB.jpeg&#39;), Path(&#39;data/covid/7EF28E12-F628-4BEC-A8C5-E6277C2E4F60.png&#39;), Path(&#39;data/covid/80446565-E090-4187-A031-9D3CEAA586C8.jpeg&#39;), Path(&#39;data/covid/F2DE909F-E19C-4900-92F5-8F435B031AC6.jpeg&#39;), Path(&#39;data/covid/nejmoa2001191_f4.jpeg&#39;), Path(&#39;data/covid/1.CXRCTThoraximagesofCOVID-19fromSingapore.pdf-000-fig1a.png&#39;), Path(&#39;data/covid/aspiration-pneumonia-5-day10.jpg&#39;), Path(&#39;data/covid/2C26F453-AF3B-4517-BB9E-802CF2179543.jpeg&#39;), Path(&#39;data/covid/all14238-fig-0002-m-d.jpg&#39;), Path(&#39;data/covid/wong-0002.jpg&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g07a-Fig7a-day5.jpeg&#39;), Path(&#39;data/covid/kjr-21-e24-g002-l-c.jpg&#39;), Path(&#39;data/covid/6b44464d-73a7-4cf3-bbb6-ffe7168300e3.annot.original.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-49-day4.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-23-day1.jpg&#39;), Path(&#39;data/covid/kjr-21-e24-g002-l-b.jpg&#39;), Path(&#39;data/covid/radiol.2020201160.fig2a.jpeg&#39;), Path(&#39;data/covid/figure1-5e71be566aa8714a04de3386-98-left.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-42.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-40.jpg&#39;), Path(&#39;data/covid/ards-secondary-to-tiger-snake-bite.png&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g07b-Fig7b-day12.jpeg&#39;), Path(&#39;data/covid/wong-0003.jpg&#39;), Path(&#39;data/covid/all14238-fig-0002-m-e.jpg&#39;), Path(&#39;data/covid/pneumocystis-pneumonia-2-PA.png&#39;), Path(&#39;data/covid/covid-19-pneumonia-22-day1-l.png&#39;), Path(&#39;data/covid/covid-19-caso-70-2-APS.jpg&#39;), Path(&#39;data/covid/1.CXRCTThoraximagesofCOVID-19fromSingapore.pdf-000-fig1b.png&#39;), Path(&#39;data/covid/parapneumonic-effusion-1-PA.png&#39;), Path(&#39;data/covid/nejmc2001573_f1b.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-20-l-on-admission.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-23-day3.jpg&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g04b-Fig4b-day12.jpeg&#39;), Path(&#39;data/covid/nejmoa2001191_f1-PA.jpeg&#39;), Path(&#39;data/covid/7AF6C1AF-D249-4BD2-8C26-449304105D03.jpeg&#39;), Path(&#39;data/covid/ryct.2020200034.fig5-day0.jpeg&#39;), Path(&#39;data/covid/35AF5C3B-D04D-4B4B-92B7-CB1F67D83085.jpeg&#39;), Path(&#39;data/covid/kjr-21-e24-g002-l-a.jpg&#39;), Path(&#39;data/covid/85E52EB3-56E9-4D67-82DA-DEA247C82886.jpeg&#39;), Path(&#39;data/covid/auntminnie-d-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg&#39;), Path(&#39;data/covid/wong-0000.jpg&#39;), Path(&#39;data/covid/pneumocystis-pneumonia-8.jpg&#39;), Path(&#39;data/covid/5931B64A-7B97-485D-BE60-3F1EA76BC4F0.jpeg&#39;), Path(&#39;data/covid/all14238-fig-0002-m-f.jpg&#39;), Path(&#39;data/covid/B2D20576-00B7-4519-A415-72DE29C90C34.jpeg&#39;), Path(&#39;data/covid/58cb9263f16e94305c730685358e4e_jumbo.jpeg&#39;), Path(&#39;data/covid/DE488FE1-0C44-428B-B67A-09741C1214C0.jpeg&#39;), Path(&#39;data/covid/ryct.2020200034.fig2.jpeg&#39;), Path(&#39;data/covid/pneumococcal-pneumonia-day35.jpg&#39;), Path(&#39;data/covid/wong-0004.jpg&#39;), Path(&#39;data/covid/cavitating-pneumonia-4-day28-PA.png&#39;), Path(&#39;data/covid/covid-19-caso-70-1-PA.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-53.jpg&#39;), Path(&#39;data/covid/F051E018-DAD1-4506-AD43-BE4CA29E960B.jpeg&#39;), Path(&#39;data/covid/kjr-21-e24-g003-l-a.jpg&#39;), Path(&#39;data/covid/1-s2.0-S1684118220300608-main.pdf-001.jpg&#39;), Path(&#39;data/covid/covid-19-pneumonia-43-day2.jpeg&#39;), Path(&#39;data/covid/F4341CE7-73C9-45C6-99C8-8567A5484B63.jpeg&#39;), Path(&#39;data/covid/2C10A413-AABE-4807-8CCE-6A2025594067.jpeg&#39;), Path(&#39;data/covid/CD50BA96-6982-4C80-AE7B-5F67ACDBFA56.jpeg&#39;), Path(&#39;data/covid/wong-0005.jpg&#39;), Path(&#39;data/covid/41591_2020_819_Fig1_HTML.webp-day5.png&#39;), Path(&#39;data/covid/covid-19-pneumonia-mild.JPG&#39;), Path(&#39;data/covid/1-s2.0-S1684118220300682-main.pdf-003-b1.png&#39;), Path(&#39;data/covid/SARS-10.1148rg.242035193-g04mr34g09b-Fig9b-day19.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-30-L.jpg&#39;), Path(&#39;data/covid/B59DD164-51D5-40DF-A926-6A42DD52EBE8.jpeg&#39;), Path(&#39;data/covid/fff49165-b22d-4bb4-b9d1-d5d62c52436c.annot.original.png&#39;), Path(&#39;data/covid/a1a7d22e66f6570df523e0077c6a5a_jumbo.jpeg&#39;), Path(&#39;data/covid/X-ray_of_cyst_in_pneumocystis_pneumonia_1.jpg&#39;), Path(&#39;data/covid/kjr-21-e24-g003-l-b.jpg&#39;), Path(&#39;data/covid/39EE8E69-5801-48DE-B6E3-BE7D1BCF3092.jpeg&#39;), Path(&#39;data/covid/C6EA0BE5-B01E-4113-B194-18D956675E25.jpeg&#39;), Path(&#39;data/covid/covid-19-rapidly-progressive-acute-respiratory-distress-syndrome-ards-admission.jpg&#39;), Path(&#39;data/covid/1-s2.0-S1684118220300608-main.pdf-002.jpg&#39;), Path(&#39;data/covid/FC230FE2-1DDF-40EB-AA0D-21F950933289.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-24-day12.jpg&#39;), Path(&#39;data/covid/171CB377-62FF-4B76-906C-F3787A01CB2E.jpeg&#39;), Path(&#39;data/covid/925446AE-B3C7-4C93-941B-AC4D2FE1F455.jpeg&#39;), Path(&#39;data/covid/covid-19-pneumonia-14-L.png&#39;), Path(&#39;data/covid/1-s2.0-S1684118220300682-main.pdf-003-b2.png&#39;)] . &#39;normal.jpg&#39;, &#39;covid_jpeg&#39;, &#39;pneumonia.jpeg&#39; . (&#39;normal.jpg&#39;, &#39;covid_jpeg&#39;) . import os [f for f in os.listdir() if &#39;jpeg&#39; in f] . [&#39;covid.jpeg&#39;, &#39;pneumonia.jpeg&#39;] . fn =&#39;normal.jpg&#39; x = PILImage.create(fn);x . dl = dls.test_dl([fn]) b = dl.one_batch() xb_im = TensorImage(dls.train.decode(b)[0][0]) xb = b[0] . hook_a,hook_g = hooked_backward() . acts = hook_a.stored[0].cpu() grad = hook_g.stored[0][0].cpu() grad_chan = grad.mean(1).mean(1) mult = (acts*grad_chan[...,None,None]).mean(0) . show_heatmap(mult) .",
            "url": "http://supriya.org/2020/04/10/covid-exp.html",
            "relUrl": "/2020/04/10/covid-exp.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Bulldozer sales Price",
            "content": "import os os.listdir() . [&#39;TrainAndValid.csv.zip&#39;, &#39;TrainAndValid.csv&#39;, &#39;.ipynb_checkpoints&#39;, &#39;Bulldozars.ipynb&#39;] . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . df = pd.read_csv(&#39;TrainAndValid.csv&#39;) . /opt/conda/envs/fastai/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . pd.options.display.max_columns = 100 . df.head() . SalesID SalePrice MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls . 0 1139246 | 66000.0 | 999089 | 3157 | 121 | 3.0 | 2004 | 68.0 | Low | 11/16/2006 0:00 | 521D | 521 | D | NaN | NaN | NaN | Wheel Loader - 110.0 to 120.0 Horsepower | Alabama | WL | Wheel Loader | NaN | EROPS w AC | None or Unspecified | NaN | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2 Valve | NaN | NaN | NaN | NaN | None or Unspecified | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Standard | Conventional | . 1 1139248 | 57000.0 | 117657 | 77 | 121 | 3.0 | 1996 | 4640.0 | Low | 3/26/2004 0:00 | 950FII | 950 | F | II | NaN | Medium | Wheel Loader - 150.0 to 175.0 Horsepower | North Carolina | WL | Wheel Loader | NaN | EROPS w AC | None or Unspecified | NaN | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2 Valve | NaN | NaN | NaN | NaN | 23.5 | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Standard | Conventional | . 2 1139249 | 10000.0 | 434808 | 7009 | 121 | 3.0 | 2001 | 2838.0 | High | 2/26/2004 0:00 | 226 | 226 | NaN | NaN | NaN | NaN | Skid Steer Loader - 1351.0 to 1601.0 Lb Operat... | New York | SSL | Skid Steer Loaders | NaN | OROPS | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Auxiliary | NaN | NaN | NaN | NaN | NaN | None or Unspecified | None or Unspecified | None or Unspecified | Standard | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 1139251 | 38500.0 | 1026470 | 332 | 121 | 3.0 | 2001 | 3486.0 | High | 5/19/2011 0:00 | PC120-6E | PC120 | NaN | -6E | NaN | Small | Hydraulic Excavator, Track - 12.0 to 14.0 Metr... | Texas | TEX | Track Excavators | NaN | EROPS w AC | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2 Valve | NaN | NaN | NaN | NaN | NaN | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 1139253 | 11000.0 | 1057373 | 17311 | 121 | 3.0 | 2007 | 722.0 | Medium | 7/23/2009 0:00 | S175 | S175 | NaN | NaN | NaN | NaN | Skid Steer Loader - 1601.0 to 1751.0 Lb Operat... | New York | SSL | Skid Steer Loaders | NaN | EROPS | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Auxiliary | NaN | NaN | NaN | NaN | NaN | None or Unspecified | None or Unspecified | None or Unspecified | Standard | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . df[&#39;SalePrice&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4f34e78450&gt; . df.select_dtypes(&#39;object&#39;).columns . Index([&#39;UsageBand&#39;, &#39;saledate&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;], dtype=&#39;object&#39;) . df[&#39;saledate&#39;] = pd.to_datetime(df[&#39;saledate&#39;]) . df[&#39;saleYear&#39;] = df[&#39;saledate&#39;].dt.year df[&#39;saleMonth&#39;] = df[&#39;saledate&#39;].dt.month . df.drop([&#39;saledate&#39;],axis=1,inplace=True) . df.shape . (412698, 54) . df[&#39;YearMade&#39;] . 0 2004 1 1996 2 2001 3 2001 4 2007 ... 412693 2005 412694 2005 412695 2005 412696 2006 412697 2006 Name: YearMade, Length: 412698, dtype: int64 . cat_cols = list(df.select_dtypes(&#39;object&#39;).columns) print(cat_cols) . [&#39;UsageBand&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;] . from sklearn.preprocessing import LabelEncoder for col in cat_cols: df[col] = pd.Categorical(df[col]).codes + 1 . df = df.fillna(df.median()) . Train, Test Split . from sklearn.model_selection import train_test_split . X_train,X_val,y_train,y_val = train_test_split(df.drop(&#39;SalePrice&#39;,axis=1),df[&#39;SalePrice&#39;]) . X_train.shape, X_val.shape . ((309523, 53), (103175, 53)) . from sklearn.ensemble import RandomForestRegressor . model = RandomForestRegressor() . model.fit(X_train.iloc[:20000], y_train.iloc[:20000]) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . model.score(X_val,y_val) . 0.840641734889723 . model = RandomForestRegressor(n_estimators=40, max_features=0.5, min_samples_leaf=3) . model.fit(X_train, y_train) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=0.5, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=3, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . model.score(X_val,y_val) . 0.9051825471683544 . Interpretation . from pdpbox import pdp, get_dataset, info_plots . X_train_samp = X_train[X_train[&#39;YearMade&#39;]&gt;1920] . pdp_year = pdp.pdp_isolate( model=model, dataset=X_train_samp, model_features=X_train_samp.columns, feature=&#39;YearMade&#39; ) fig, axes = pdp.pdp_plot(pdp_year, &#39;YearMade&#39;) . findfont: Font family [&#39;Arial&#39;] not found. Falling back to DejaVu Sans. findfont: Font family [&#39;Arial&#39;] not found. Falling back to DejaVu Sans. findfont: Font family [&#39;Arial&#39;] not found. Falling back to DejaVu Sans. findfont: Font family [&#39;Arial&#39;] not found. Falling back to DejaVu Sans. . X_train_samp.shape . (279942, 53) . import shap . explainer = shap.TreeExplainer(model) . X_train_less = X_train_samp.sample(500) . X_train_less.shape . (500, 53) . shap_values = explainer.shap_values(X_train_less) . shap.summary_plot(shap_values, X_train_less) . # shap.summary_plot(shap_values, X_train_less, plot_type=&quot;bar&quot;) . %%time inter_rf = pdp.pdp_interact( model=model, dataset=X_train_less, model_features=X_train_less.columns, features=[&#39;YearMade&#39;, &#39;saleYear&#39;] ) . CPU times: user 2.39 s, sys: 16.1 ms, total: 2.4 s Wall time: 2.4 s . fig, axes = pdp.pdp_interact_plot( inter_rf, [&#39;YearMade&#39;, &#39;saleYear&#39;], x_quantile=True, plot_type=&#39;grid&#39;, plot_pdp=True, figsize=(13,13) ) .",
            "url": "http://supriya.org/2020/04/08/bulldozers.html",
            "relUrl": "/2020/04/08/bulldozers.html",
            "date": " • Apr 8, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides has for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Feautures . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "http://supriya.org/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "http://supriya.org/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am passionate about Data science and have extensive knowledge in Machine learning, Deep learning, computer vision and Cloud deployment. I am eager to learn and explore new skills and try to keep my skills right on the cutting edge. .",
          "url": "http://supriya.org/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}